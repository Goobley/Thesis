\chapter{Optical Flare Observations and Inversions}\label{Chap:FlareObservations}
% spell-checker: disable
%TC:group pycode 0 0
%TC:group tikzpicture 0 0
\begin{pycode}[FlareObs]
name = 'FlareObs'
chFlareObs = texfigure.Manager(
    pytex,
    './01bFlareObservations',
    number=1,
    python_dir='./01bFlareObservations/python',
    fig_dir=   './01bFlareObservations/Figs',
    data_dir=  './Data/01aFlareModelling'
)
\end{pycode}
% spell-checker: enable

% \begin{itemize}
%     \item How flares are observed.
%     \item Important optical spectral lines \Ha{}, \CaLine{}; observations thereof.
%     \item Forward Modelling
%     \item Inversions
%     \item Machine Learning
% \end{itemize}

Solar flares release a vast quantity of energy across the entire electromagnetic spectrum, observed from $\gamma$-rays to microwaves.
Almost all of this presents significant diagnostic potential, and is observed by a plethora of advanced instruments.
Many of these have to be situated above the Earth's atmosphere, which shields us from the biologically harmful wavelengths at which they observe.
An in-depth review of different spectral ranges, and the flaring signatures they observe is given in \citet{Fletcher2011}.

In this chapter we will discuss the optical spectral lines that we focus on during this thesis, the basics of inversion (retrieving a solar atmosphere from spectral line observations), and finally introduce the concepts used in machine learning that will be applied in our machine learning inversion model presented in Chap.~\ref{Chap:Radynversion}.

\section{Important Optical Spectral Lines}

% spell-checker: disable
\begin{pycode}[FlareObs]
from lightweaver.rh_atoms import CaII_atom, H_4_atom
import lightweaver as lw
# NOTE(cmo): This is adapted from the default Lw version for thesis style/adjustability.
def grotrian_diagram(atom : 'AtomicModel', ax=None, orbitalLabels=None,
                     vacuumWavelength=True, highlightLines=None, adjustTextPositions=False,
                     labelLevels=True, wavelengthLabels=True, continuumEdgeLabels=False,
                     boundOnly=False, fontSize=9, suppressLineLabels=None, rotateLabels=True,
                     lineLabelYLocs=None):
    '''
    Produce a Grotrian (term) diagram for a model atom.

    Parameters
    ----------
    atom : AtomicModel
        The model atom for which to produce the diagram.
    ax : matplotlib.axis.Axes, optional
        The axes on which to create the plot.
    orbitalLabels : Optional[List[str]]
        The labels for the oribtals on the x-axis.
    vacuumWavelength : bool
        Whether to plot the wavelengths with their vacuum values, or convert
        to air. Default: True i.e. vacuum.
    highlightLines : Optional[List[int]]
        The indices of lines in atom.lines to highlight. These will be
        plotted in red, whereas the others will be plotted in green.
    adjustTextPositions: bool
        Whether to move the wavelength labels around to avoid the level
        labels and each other. Default: False.
    labelLevels: bool or List[str]
        Whether to label the levels (parsing the names for these from
        atom.levels[i].label if True), or uses the provided strings (one per
        level) if a List[str], or not label at all (False). Default: True.
    wavelengthLabels : bool
        Whether to label the wavelengths for each bound-bound transition.
        Default: True.
    continuumEdgeLabels : bool
        Label the continuum edge wavelengths (only if wavelengthLabels also True).
        Default: False.
    boundOnly : bool
        Whether to only show the bound terms of the model.
        Default: False.
    fontSize: float
        Font size to use for the annotations. Default: 9
    suppressLineLabels : Optional[List[int]]
        List of line indices to not print wavelength labels for. Default: None.
    rotateLabels : bool
        Whether to rotate the wavelength labels to lie along the line
        transitions, does not work well with `adjustTextPositions`. Default: True
    '''
    from typing import List, Sequence
    import lightweaver.constants as C
    import matplotlib.pyplot as plt
    from copy import copy
    if adjustTextPositions:
        from adjustText import adjust_text

    if ax is None:
        ax = plt.gca()

    if highlightLines is None:
        highlightLines = []

    orbits = ['S', 'P', 'D', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'Q', 'R', 'T', 'U', 'V', 'W', 'X']

    levels = [copy(l) for l in atom.levels]
    levelTrueEEv = [level.E_eV for level in atom.levels]
    # NOTE(cmo): Just assume that it is, even if it has a defined L (e.g. for
    # polarisation reasons)
    # if atom.levels[-1].L is not None:
    #     raise ValueError('Last level should be ground term')

    ls = sorted(list(set([l.L for l in atom.levels[:-1]])))

    avoidLevelLabels = []
    # NOTE(cmo): Draw levels in
    MinLevelGap = 0.2
    for currentL in ls:
        currentEnergies = []
        for level in levels[:-1]:
            if level.L == currentL:
                e = level.E_eV
                if len(currentEnergies) != 0:
                    energyDiff = np.array([e - ee for ee in currentEnergies])
                    idx = np.argmin(np.abs(energyDiff))
                    if np.abs(energyDiff[idx]) < MinLevelGap:
                        e = currentEnergies[idx] + np.sign(energyDiff[idx]) * MinLevelGap
                ax.plot([level.L-0.25, level.L+0.25], [e, e], c='k')
                currentEnergies.append(e)
                level.E = e * C.EV * C.CM_TO_M / C.HC


    texts = []
    if not boundOnly:
        # NOTE(cmo): Draw overlying cont
        ax.plot([min(ls)-0.25, max(ls)+0.25], [levels[-1].E_eV, levels[-1].E_eV], '--', c='C0')

        # NOTE(cmo): Draw b-f
        contPerCol = {L: 0 for L in ls}
        for cont in atom.continua:
            lu = levels[cont.j]
            ll = levels[cont.i]
            nc = contPerCol[ll.L]
            contPerCol[ll.L] += 1
            xLoc = ll.L + 0.05 * nc
            ax.annotate("", xy=(xLoc,lu.E_eV), xytext=(xLoc, ll.E_eV), arrowprops={'arrowstyle':'->','color':'y'})
            if wavelengthLabels and continuumEdgeLabels:
                textX = ll.L
                # textY = lu.E_eV - 0.05 * (ll.E_eV + lu.E_eV)
                textY = lu.E_eV - 0.5
                lambdaEdge = cont.lambdaEdge if vacuumWavelength else vac_to_air(cont.lambdaEdge)
                a = ax.annotate('%.2f' % lambdaEdge, xy=(textX, textY),
                                fontsize=fontSize, ha='center')
                texts.append(a)


    ax.relim()
    ax.autoscale_view()


    # NOTE(cmo): Draw b-b
    for idx, line in enumerate(atom.lines):
        lu = levels[line.j]
        ll = levels[line.i]
        lineColor = 'r' if idx in highlightLines else 'k'
        ax.annotate("", xy=(ll.L,ll.E_eV), xytext=(lu.L, lu.E_eV),arrowprops={'arrowstyle':'->','color': lineColor, 'alpha':0.5})
        if wavelengthLabels:
            if suppressLineLabels is not None and idx in suppressLineLabels:
                continue
            textX = 0.5 * (ll.L + lu.L)
            textY = 0.5 * (ll.E_eV + lu.E_eV)
            lambda0 = line.lambda0 if vacuumWavelength else vac_to_air(line.lambda0)
            if rotateLabels:
                angle = np.rad2deg(np.arctan2(lu.E_eV - ll.E_eV, lu.L - ll.L))
                if angle > 90:
                    angle -= 180.0
                offset = max(0.01 * max(ax.get_ylim()), 0.04)
                if lineLabelYLocs:
                    textY = lineLabelYLocs[idx]
                else:
                    textX += offset
                    textY -= offset
                a = ax.text(textX, textY, '%.2f nm' % lambda0,# xy=(textX, textY),
                                fontsize=fontSize, rotation=angle, transform_rotates_text=True, ha='center', va='center')
            else:
                if lineLabelYLocs:
                    textY = lineLabelYLocs[idx]
                a = ax.annotate('%.2f nm' % lambda0, xy=(textX, textY),
                                fontsize=fontSize)
            texts.append(a)

    # NOTE(cmo): Label levels on top
    if labelLevels:
        if not isinstance(labelLevels, Sequence):
            labelsUsed = []
            for i, level in enumerate(levels[:-1]):
                endIdx = [level.label.upper().rfind(x) for x in ['E', 'O']]
                maxIdx = max(endIdx)
                if maxIdx == -1:
                    raise ValueError("Unable to determine parity of level %s" % (repr(level)))
                label = level.label[:maxIdx+1].upper()
                words: List[str] = label.split()
                label = ' '.join(words[:-1])
                if any(label == l for l in labelsUsed):
                    continue
                labelsUsed.append(label)
                offsetY = min(max(0.04 * max(ax.get_ylim()), 0.05), 0.5)
                offsetX = min(max(0.1 * max(ax.get_ylim()), 0.2), 0.4)
                labelY = level.E_eV - offsetY if level.E_eV != 0 else level.E_eV + offsetY / 2
                labelX = level.L - offsetX if level.E_eV != 0 else level.L + offsetX / 3
                a = ax.annotate(label, xy=(labelX, labelY), fontsize=fontSize)
                avoidLevelLabels.append(a)
            if not boundOnly:
                a = ax.annotate(levels[-1].label, xy=(-0.25, levels[-1].E_eV + 0.04), color='b', fontsize=fontSize)
                # bbox={'boxstyle': 'round,pad=0.3', 'fc': 'w', 'alpha': 0.2})
                avoidLevelLabels.append(a)
        else:
            for i, level in enumerate(levels[:-1]):
                offsetY = min(max(0.04 * max(ax.get_ylim()), 0.1), 0.5)
                offsetX = min(max(0.1 * max(ax.get_ylim()), 0.2), 0.4)
                labelY = level.E_eV - offsetY if level.E_eV != 0 else level.E_eV + offsetY / 2
                labelX = level.L - offsetX if level.E_eV != 0 else level.L + offsetX / 3
                a = ax.annotate(labelLevels[i], xy=(labelX, labelY), fontsize=fontSize)
                avoidLevelLabels.append(a)
            a = ax.annotate(labelLevels[-1], xy=(-0.25, levels[-1].E_eV + 0.04), fontsize=fontSize, bbox={'boxstyle': 'round,pad=0.3', 'fc': 'w', 'alpha': 0.2})
            avoidLevelLabels.append(a)


    if adjustTextPositions and wavelengthLabels:
        adjust_text(texts, autoalign='xy', expand_text=(1.05, 1.05),
                    force_text=(0.05, 0.05), arrowprops={'arrowstyle': 'wedge'},
                    # bbox={'boxstyle': 'round,pad=0.3', 'fc': 'g', 'alpha': 0.2},
                    # avoid_self=False,
                    add_objects=avoidLevelLabels, lim=200 )

    if orbitalLabels is None:
        orbitalLabels = {}
        labelled = []
        for level in levels[:-1]:
            if level.L in labelled:
                continue
            labelled.append(level.L)
            label = level.label
            split = label.split()
            idx = len(split) - 1
            while idx >= 0:
                if len(split[idx]) > 1:
                    break
                idx -= 1

            orbitalLabels[level.L] = split[idx]
        orbitalLabels = [orbitalLabels[k] for k in sorted(orbitalLabels.keys())]
    if len(ls) != len(orbitalLabels):
        raise ValueError('Length of orbital labels does not match provided number of different Ls in the model atom')
    ax.set_xticks(ls)
    ax.set_xticklabels(orbitalLabels)

    ax.set_ylabel('Energy [eV]')

ca = CaII_atom()

if False:
    h = H_4_atom()
    fig, ax = plt.subplots(1, 2, figsize=texfigure.figsize(pytex, scale=1))
    grotrian_diagram(h, ax=ax[0], continuumEdgeLabels=False, wavelengthLabels=True, fontSize=9, rotateLabels=True, adjustTextPositions=False, highlightLines=[2], boundOnly=True, lineLabelYLocs=[5.65,6.45,10.85], orbitalLabels=[r'S', r'P', r'D'], labelLevels=['1s', '2p', '3d'])
    grotrian_diagram(ca, ax=ax[1], continuumEdgeLabels=False, wavelengthLabels=True, fontSize=9, rotateLabels=True, adjustTextPositions=False, highlightLines=[4], boundOnly=True, lineLabelYLocs=[1.3,1.85,2.25,2.5,2.75], orbitalLabels=[r'$^2$S', r'$^2$P', r'$^2$D'], labelLevels=['4s', '', '3d', '', '4p'])
    ax[1].set_ylabel('')
    fig.tight_layout()
    lFig = chFlareObs.save_figure('Grotrians', fig, fext='.pgf')
    lFig.caption = r'Grotrian (term) diagram of primary H and \Caii{} bound terms of interest. The \Ha{} and \CaLine{} transitions are highlighted in red.'
    lFig.short_caption = r'Grotrian (term) diagram of primary H and \Caii{} bound terms of interest.'
else:
    fig = plt.figure(figsize=texfigure.figsize(pytex, scale=1))
    ax = fig.gca()
    grotrian_diagram(ca, ax=ax, continuumEdgeLabels=False, wavelengthLabels=True, fontSize=9, rotateLabels=True, adjustTextPositions=False, highlightLines=[4], boundOnly=True, lineLabelYLocs=[1.4,1.8,2.3,2.5,2.7], orbitalLabels=[r'$^2$S', r'$^2$P', r'$^2$D'], labelLevels=['4s', '', '3d', '', '4p'])
    lFig = chFlareObs.save_figure('Grotrians', fig, fext='.pgf')
    lFig.caption = r'Grotrian (term) diagram of \Caii{} bound terms of interest. The \CaLine{} transition is highlighted in red. The wavelength of each bound-bound transition is labelled.'
    lFig.short_caption = r'Grotrian (term) diagram of \Caii{} bound terms of interest.'

\end{pycode}
\py[FlareObs]|chFlareObs.get_figure('Grotrians')|
% spell-checker: enable

Ground-based telescopes can have much larger apertures and be more complex than their space-based counterparts for an equivalent budget, allowing for much better spatial resolution.
The instruments attached to them can also be repaired, replaced, and upgraded over time, allowing them to have much longer lifecycles than space missions.
In exchange for this they can only observe in limited spectral bands (due to the aforementioned atmospheric absorption effects), and suffer from the effects of atmospheric turbulence.
To some degree the effects of atmospheric seeing can be accounted for and corrected, but the magnitude of these effects will vary on a daily basis.
Our aim is to develop predictions and techniques allowing the more thorough exploitation of the high spatially and spectrally resolved data obtained from the current and next-generation of ground-based optical telescopes to enhance our understanding of the chromosphere during solar flares.

There are but a handful of spectral lines that are accessible to these ground-based telescopes and that are sensitive to the chromosphere.
Following \citet{DelaCruzRodriguez2017} these are primarily the \Caii{} H \& K lines, the \Caii{} infrared triplet, \Ha{}, He\,\textsc{i} D3, and He\,\textsc{i} \SI{1083}{\nano\metre}.
Many other spectral lines with chromospheric diagnostic potential, such as Ly$\alpha$, Mg\,\textsc{ii} h \& k, He\,\textsc{ii} \SI{30.4}{\nano\metre} can only be observed from space.
The decision regarding which of these to focus on depends both on the availability of observations (current and future) and the complexity of modelling.
The \Ha{} line at \SI{656.3}{\nano\m} has long been exploited.
It has a strong line core and wide wings with a varying degree of central reversal and asymmetry \citep[e.g.][]{Svestka1966}.
These features suggest that the line responds differently to different flares, suggesting that it has good diagnostic potential.
Indeed, \Ha{} has been a component of many investigations into flare dynamics and evolution \citep[e.g.][]{Acton1982,Heinzel1994,Wang1995,Kuridze2015,RubioDaCosta2016}.
Other lines in the hydrogen Balmer series can also be used, optionally in conjunction with \Ha{}, such as in the work of \citet{Capparelli2017}.
The spectral lines of \Caii{} have long been present in the \Radyn{} code and used for chromospheric diagnostics \citep[e.g.][]{Carlsson1992a}, and are a mainstay of flaring chromospheric analysis \citep[e.g.][]{1997Mein,Cauzzi2008,Kuridze2015,RubioDaCosta2016,Kuridze2018,Vissers2021,Yadav2021}, thanks in part to the polarisability of the \CaLine{} line.
There have been relatively few observations of the neutral helium lines at high spatial and spectral resolution in flares \citep[e.g.][]{Zeng2014, Libbrecht2019}, but they appear to have strong diagnostic potential, although it is likely necessary to exploit spectropolarimetric information to fully interrogate these lines \citep{Libbrecht2019}.
Recently unpolarised RHD modelling efforts have started to investigate the possible dimming of these lines \citep{Kerr2021}.

In the following we shall therefore focus primarily on two of the strongest optical lines, \Ha{}, and \CaLine{}, which have a long history of use within RHD modelling, and as will be discussed, can often be observed with the same instrument.
\Ha{} is the first of the hydrogen Balmer series, and is emitted by an electron transitioning between the $n=3$ and $n=2$ bound levels of hydrogen.
\CaLine{} is part of the \Caii{} infrared triplet, and is highlighted in red on the Grotrian diagram in Fig.~\ref{Fig:Grotrians}.
The \CaLine{} line is the most polarisable of this triplet (as it has the largest Landé factor of the three): it is not blended with other lines, and lies relatively close to the peak of the solar spectrum, providing a high flux for easy integration.
The pair of transitions on this same figure, between the 4p$^2$P and 4s$^2S$ levels, are the \Caii{} H \& K lines.

The \Ha{} and \CaLine{} lines have been extensively used together for chromospheric analysis and carry complementary information.
Simulations have found them to form at different heights within the chromosphere, for example \citet{Kuridze2015} found the \Ha{} line core forming in the 1.1--\SI{1.2}{\mega\metre} region of a \Radyn{} simulation, whereas the \CaLine{} line core was formed over a much more compact region, deeper in the atmosphere, around an altitude of \SI{0.9}{\mega\m}.
Diagnostics involving both \Ha{} and \CaLine{} can therefore reliably constrain chromospheric models, and are sensitive to a larger region of the solar atmosphere than is possible with just one of the two.
Both of these lines have been reliably modelled without the need for additional time-consuming PRD calculations, which is not the case for the \Caii{} H \& K lines.
Whilst \Ha{} can be treated in CRD, \citet{Leenaarts2012a} find that three-dimensional radiative transfer modelling is needed to correctly reproduce the expected intensity structure from RMHD models.
\citet{Bjorgen2019} found that the magnitude of this effect was reduced for the brightest regions of a three-dimensional active region simulation \citep{Cheung2019}, due to the localised high chromospheric mass density, but three-dimensional modelling remained important for regions adjacent to these bright structures.
Flare models with detailed chromospheric treatment are not yet possible in three-dimensions, but with the large gradients in the radiation field that occur in flares it is likely that this effect will also be important in regions adjacent to flares (as supported by the simulations presented in Chap.~\ref{Chap:2DRT}).


\subsection{The Swedish Solar Telescope}\label{Sec:CRISP}

The Swedish Solar Telescope \citep[SST,][]{Scharmer2003} is a 1-metre refracting telescope located in the Observatoria del Roque de los Muchachos, La Palma, Spain.
Behind the singlet lens, the telescope is held in a vacuum to improve image quality and a Schupmann corrector is used to compensate for chromatic aberration.
The light is then sent from this corrector to an adaptive optics system that compensates for atmospheric seeing effects and on to the optical bench.
This adaptive optics system allows the SST to operate at close to its diffraction limit in good seeing conditions ($\sim 0.17^{\prime\prime}$ for \Ha{} and $\sim 0.21^{\prime\prime}$ for \CaLine{}). % Rayleigh criterion

There are three instruments usable with the SST: TRIPPEL, CHROMIS, and CRISP.
TRI-Port Polarimetric Echelle-Littrow \citep[TRIPPEL,][]{Kiselman2011} is a spectrograph that can simultaneously observe in three different wavelength regions (across 380--\SI{1100}{\nano\m}) with a spectral resolution\footnote{\added{The spectral resolution $R$ describes an instrument's ability to resolve spectral features and is defined by $\cfrac{\lambda}{\Delta\lambda}$ where for a wavelength $\lambda$, $\Delta\lambda$ is the smallest difference in wavelengths that can be distinguished.}} $R\approx200\,000$.
CHROMospheric Imaging Spectrometer \citep[CHROMIS,][]{Lofdahl2021} and CRisp Imaging SpectroPolarimeter \citep[CRISP,][]{Scharmer2008,Scharmer2019} are both dual Fabry-Pérot tunable filter systems that can be operated simultaneously.
CHROMIS serves the blue end of the spectrum between 380 and \SI{500}{\nano\m}, whereas CRISP operates in the 510--\SI{860}{\nano\m} region.
The diffraction limit is lower at the CHROMIS wavelengths, but CRISP can perform full spectropolarimetric imaging.

In this thesis we make use of one set of observations taken from the SST using the CRISP instrument, so we shall briefly describe how these are processed.
This observation was performed on 2014-09-06 and captures the M1.1 flare SOL 20140906T17:09 from National Oceanic and Atmospheric Administration (NOAA) Active Region 12157 at heliocentric coordinates ($-732^{\prime\prime},\,-302^{\prime\prime}$).
This data is available in the F-CHROMA database\footnote{\url{https://star.pst.qub.ac.uk/wiki/public/solarflares/0450.html}} and was prepared using the CRISPRED pipeline \citep{DelaCruzRodriguez2015} which is responsible for image calibration, alignment, instrumental effects, and seeing restoration through the multi-object multi-frame blind deconvolution algorithm \citep[MOMFBD,][]{VanNoort2005}.
MOMFBD is a \added{post-processing} phase diversity algorithm \replaced{that accounts}{ needed to account} for seeing that is worse, or evolving faster, than the adaptive optics system of the telescope can cope with.
It has been suggested by \citet{Armstrong2021} that the differences between the wide- and narrowband images of flares may hinder the reconstructive abilities of the MOMFBD system, and they have developed a machine learning approach to correcting for these effects.
This method has not been applied to the data used in this thesis.

\subsection{The Daniel K Inouye Solar Telescope}

The Daniel K Inouye Solar Telescope \citep[DKIST,][]{Rimmele2020} is a new 4-metre Gregorian solar telescope that saw first solar light in December 2019.
It is designed to support a number of instruments observing between 380 and \SI{5000}{\nano\metre} with a diffraction limit of $0.026^{\prime\prime}$ (\SI{20}{\kilo\metre}) at \SI{500}{\nano\metre}.
The instruments initially expected to capture flaring chromospheric observations are the Visible Tunable Filter (VTF) and the Visible Spectro-Polarimeter (ViSP).
The former of these will capture narrow-band line images similar to CRISP, but at higher spatial resolution, whereas the latter will provide high-precision spectropolarimetric observations through a slit spectrograph with $R>180\,000$.
DKIST is expected to start making routine observations in 2021 and promises observations of the chromosphere on scales that will push our modelling and analysis techniques to their limits.

\section{Introduction to Inverse Problems}\label{Sec:InverseProblems}

From the previous chapters we have an understanding of the so-called ``forward problem'' of radiative transfer, that is, the synthesis of a spectrum from a known atmospheric model.
The inverse of this problem is not well-posed; there is no guarantee of uniqueness, and typically the problem is extremely underdetermined.
The radiation field inside the plasma couples the atomic populations at all depths (as is evident from the form of the $\Lambda$ operator) in a way that cannot be trivially disentangled and information is then lost in the forward process that is needed for the inverse process.

It is, of course, of great value to constrain the atmospheric parameters associated with a particular observation, and it is this we seek to achieve, by solving the inverse problem of radiative transfer, known as inversion.
As observed radiation is the only vector by which information can arrive from the Sun, it is important to maximally exploit the information that can be gleaned from observations and determine the structure of the atmosphere that produced the observed radiation.

\begin{figure}
\centering
\begin{tikzpicture}[line width=1pt, >=latex]
    \node (x1) {$T_1,\rho_1,\vec{\varv}_1, \vec{B}_1$};
    \node[below=of x1] (x2) {$T_2,\rho_2,\vec{\varv}_2, \vec{B}_2$};
    \node[below=of x2] (x3) {$T_3,\rho_3,\vec{\varv}_3, \vec{B}_3$};
    \node[below=of x3] (x4) {$T_4,\rho_4,\vec{\varv}_4, \vec{B}_4$};

    \node[above right=0.5cm and 4cm of x1] (y1) {$\vec{I}_1$};
    \node[below=of y1] (y2) {$\vec{I}_2$};
    \node[below=of y2] (y3) {$\vec{I}_3$};
    \node[below=of y3] (y4) {$\vec{I}_4$};
    \node[below=of y4] (y5) {$\vec{I}_5$};
    % \node[right=4cm of x4] (aux2) {};

    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(x1) (x4)}] (xSet) {};
    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(y1) (y5)}] (ySet) {};

    \node[below=2cm of y5,font=\bfseries, align=center] (yLabel) {$\mathcal{Y}$\\Observed Intensity};
    % \node[left=3cm of ylabel.center,font=\bfseries, align=center] {$\mathcal{X}$\\Atmospheric Parameters};
    \node[font=\bfseries, align=center] (xLabel) at (xSet |- yLabel){$\mathcal{X}$\\Atmospheric Parameters};

    % \draw[->] (xSet.90) to [out=50, in=150] (ySet.90) node[midway] {aa $f(\vec{x})$};
    % \draw[->, dotted] (ySet.270) to [out=210, in=310] (xSet.270) node[midway] {a $f^{-1}(\vec{x})$};
    \draw[->, out=50, in=150] (xSet.90) to node[below] {$f(x)$} (ySet.90);
    \draw[->, dotted, out=210, in=310] (ySet.270) to node[above] {$f^{-1}(y)$} (xSet.270);

    \draw[->] (x1) -- (y2.170);
    \draw[->] (x2) -- (y4.190);
    \draw[->, dotted] (y2.150) -- (x1.5) node[midway, above] {?};
    \draw[->, dotted] (y2.200) -- (x3.20) node[midway, below] {?};
    \draw[->] (x3) -- (y2.175);
    \draw[->] (x4) -- (y5.190);
\end{tikzpicture}
\caption[Diagram illustrating the degeneracy present in traditional inversions.]{The degenerate mapping $f$ demonstrates the difficulty of traditionally framed inversions.}
\label{Fig:DegenerateMapping}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}[line width=1pt, >=latex]
    \node (x1) {$T_1,\rho_1,\vec{\varv}_1, \vec{B}_1$};
    \node[below=of x1] (x2) {$T_2,\rho_2,\vec{\varv}_2, \vec{B}_2$};
    \node[below=of x2] (x3) {$T_3,\rho_3,\vec{\varv}_3, \vec{B}_3$};
    \node[below=of x3] (x4) {$T_4,\rho_4,\vec{\varv}_4, \vec{B}_4$};
    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(x1) (x4)}] (xSet) {};

    \node[above right=1cm and 3.5cm of xSet.center] (y1) {$(\vec{I}_2, z_1)$};
    \node[below=of y1] (y2) {$(\vec{I}_4, z_1)$};
    \node[below=of y2] (y3) {$(\vec{I}_5, z_1)$};
    \node[right=0.5cm of y1] (y4) {$(\vec{I}_2, z_2)$};
    \node[below=of y4] (y5) {$(\vec{I}_4, z_2)$};
    \node[below=of y5] (y6) {$(\vec{I}_5, z_2)$};
    \node[right=0.5cm of y4] (y7) {$(\vec{I}_2, z_3)$};
    \node[below=of y7] (y8) {$(\vec{I}_4, z_3)$};
    \node[below=of y8] (y9) {$(\vec{I}_5, z_3)$};
    % \node[right=4cm of x4] (aux2) {};

    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(y1) (y5) (y9)}] (ySet) {};

    \node[shape=circle, draw=black, right=2cm of y8] (prod) {$\times$};
    \draw[->, ultra thick] (prod.180) -- (ySet.0);

    \node[above=2cm of prod] (yy3) {$\vec{I}_5$};
    \node[above=0.5cm of yy3] (yy2) {$\vec{I}_4$};
    \node[above=0.5cm of yy2] (yy1) {$\vec{I}_2$};
    \node[shape=ellipse,draw=black,minimum size=1cm,fit={(yy1) (yy3)}] (yySet) {};
    \draw[->, ultra thick] (yySet.270) -- (prod.90);

    \node[below=2cm of prod] (z1) {$z_1$};
    \node[below=0.5cm of z1] (z2) {$z_2$};
    \node[below=0.5cm of z2] (z3) {$z_3$};
    \node[shape=ellipse,draw=black,minimum size=1cm,fit={(z1) (z3)}] (zSet) {};
    \draw[->, ultra thick] (zSet.90) -- (prod.270);

    \node[below=6cm of ySet.center,font=\bfseries, align=center] (yLabel) {$\mathcal{Y}\times\mathcal{Z}$\\Observed Intensity $\times$ Latent Space};
    \node[font=\bfseries, align=center] (xLabel) at (xSet |- yLabel){$\mathcal{X}$\\Atmospheric Parameters};
    \node[font=\bfseries, align=center] (zLabel) at (zSet |- yLabel){$\mathcal{Z}$\\Latent Space};
    \node[font=\bfseries, align=center, above=0.2cm of yySet.north] (yyLabel) {$\mathcal{Y}$\\Observed\\Intensity};

    % \node[below=2cm of y5,font=\bfseries, align=center] (ylabel) {$\mathcal{Y}$\\Observed Intensity};
    % \node[left=3cm of ylabel.center,font=\bfseries, align=center] {$\mathcal{X}$\\Atmospheric Parameters};

    % \draw[->] (xSet.90) to [out=50, in=150] (ySet.90) node[midway] {aa $f(\vec{x})$};
    % \draw[->, dotted] (ySet.270) to [out=210, in=310] (xSet.270) node[midway] {a $f^{-1}(\vec{x})$};
    \draw[->, out=50, in=100] (xSet.90) to node[below] {$g^{-1}(x)$} (ySet.90);
    \draw[->, dotted, out=260, in=310] (ySet.270) to node[above] {$g(y, z)$} (xSet.270);

    \draw[<->] (x1) -- (y4);
    \draw[<->] (x2) -- (y2);
    % \draw[->, dotted] (y2.150) -- (x1.5) node[midway, above] {?};
    % \draw[->, dotted] (y2.200) -- (x3.20) node[midway, below] {?};
    \draw[<->] (x3) -- (y7);
    \draw[<->] (x4.350) -- (y6.190);


\end{tikzpicture}
\caption[Resolving the degeneracy of inversions through the addition of a latent space.]{The bijective mapping $g$ can be used to resolve the problem of inversions upon the introduction of a latent space $\mathcal{Z}$ containing the information lost in the forward process. For the sake of legibility we have only included the observed intensity vectors which were mapped to in Fig.~\ref{Fig:DegenerateMapping}.}
\label{Fig:BijectiveMapping}
\end{figure}

The forward process is described by the diagram in Fig.~\ref{Fig:DegenerateMapping}.
Here elements of $\mathcal{X}$ are the parameters describing an atmosphere (in this description we choose temperature $T$, mass density $\rho$, velocity $\vec{\varv}$, and magnetic field $\vec{B}$, although there are many other similar formulations that can be used).
The elements of $\mathcal{Y}$ are possible forms of the outgoing radiation that may be associated with different sets of atmospheric parameters.
Fig.~\ref{Fig:DegenerateMapping} shows the theoretically degenerate nature of the problem; whilst the mapping $f$ from $\mathcal{X}$ to $\mathcal{Y}$ is always well-posed, the inverse $f^{-1}$ is not necessarily, and with only the information present in $\mathcal{Y}$ there is no way to immediately resolve this ambiguity (indicated by the question marks in this figure).

The problem of inversion can instead be framed as shown in Fig.~\ref{Fig:BijectiveMapping}, where elements of the newly introduced latent space $\mathcal{Z}$ represent the information lost in the forward process and a bijective mapping $g$ may be written between $\mathcal{X}$ and the cartesian product of $\mathcal{Y}$ and $\mathcal{Z}$.
$\mathcal{Z}$ is difficult to conceptualise, and harder still to characterise; in the following we will discuss multiple approaches for replacing or reconstructing this information.

As an example we can pose the simple problem of the function $f : \mathbb{R} \rightarrow \mathbb{R}^+; x \mapsto x^2$. This function is clearly not bijective, as for any $y$ in $\mathbb{R}^+$ it is ambiguous whether the expected input $x$ was $\sqrt{y}$ or $-\sqrt{y}$.
If the domain of this function was instead $\mathbb{R}^+$, there would be no ambiguity here.
Instead, we can introduce a latent space, the form of which is chosen by us and captures the lost information, i.e. the sign of the input ($\{1,\,-1\}$).
With this we can define the bijective function $g : \mathbb{R}^+\times\,\{1,\,-1\} \rightarrow \mathbb{R}; (y, z) \mapsto z \sqrt{y}$.
Discarding $z$, the forward process of $g^{-1}$ is equivalent to that described by $f$, but the invertibility of the problem is now assured.
This example does not illustrate how to discover the correct $z$, and for a purely mathematical case such as this, there is no unambiguous solution given only $y$.
However, the form of the latent space is known (and in this case finite), and this can allow us to infer the different possibilities for $x$.
In complex physical systems these different possibilities may have different likelihoods of occurring that then allow us to construct a probability distribution function for $x$.

For the remainder of this chapter, we denote individual samples of the atmospheric parameters $x$, the emergent line profiles $y$ and the latent space $z$.

\subsection{Milne-Eddington Inversions}

\emph{In depth reviews of the primary methods of spectral inversion are provided by \citet{DelToroIniesta2016} and \citet{DelaCruzRodriguez2017}.}

Milne-Eddington inversions have proven to be a simple, but very powerful tool that is often applied in solar physics.
The loss of information can be somewhat limited by placing constraints on the solution. One of the simplest constraints that can be placed on the problem is that of a source function that changes linearly with continuum optical depth, whilst all other parameters are held constant throughout the atmosphere.
This is a low-order approximation of the problem, and is convenient as we can express the outgoing intensity analytically.
This can be done for the full Stokes polarised case of the RTE, but for illustration we choose to use only the scalar case here.
With continuum opacity $\tau_c$ the source function is then defined as
\begin{equation}
    S(\tau_c) = S_0 + S_1 \tau_c,
\end{equation}
where $S_0$ and $S_1$ are constants.
In this situation where we are dealing with the source function directly, rather than atomic parameters (as we are effectively in an two-level atom case due to only considering a single line), we can define the line strength $\alpha$ as the ratio between the continuum opacity and the line opacity i.e.
\begin{equation}
    \tau(\nu) = \tau_c (1 + \alpha \phi(\nu)),
\end{equation}
where $\phi$ is the line profile.

Now, by \added{formulating $S$ in terms of $\tau(\nu)$ and} integrating the RTE directly whilst assuming a semi-infinite atmosphere \deleted{with no light entering the boundary at $\infty$} we obtain the outgoing intensity
\begin{align}
I(\tau=0, \nu) &= \int_0^\infty \left(S_0 + S_1 \frac{\tau}{1+\alpha\phi(\nu)}\right) e^{-\tau} \mathop{\dd{}\tau}\\
               &= S_0 + \frac{S_1}{1 + \alpha\phi(\nu)}.
\end{align}

Thanks to the analytic nature of this solution, it is easy to attempt to fit this to observations, and effects such as constant atmospheric velocity, and magnetic field can be included in the line profile.
This approach is rapid and works quite well for quiet photospheric lines, such as Fe I 6301 \AA{} and Fe I 6173 \AA{} as used in Hinode SOT and HMI \citep[e.g.][]{Centeno2014}, but the linear description of source function reduces its applicability to chromospheric lines.
\NOTE{It has regardless been applied to chromospheric lines at times.}

\subsection{Generalisation Through Response Functions}\label{Sec:IntroRfs}


It can be very limiting to impose a chosen source function, especially in regions where the temperature does not vary monotonically.
A far more flexible approach is to attempt to fit a source function.
From the previous discussion of formal solvers we know that for a discretised atmosphere wherein the source function follows a prescribed functional form over each interval we can write
\begin{equation}
    I(\tau=0) = \sum_i \int_{\tau_i}^{\tau_{i+1}} S(\tau)e^{-\tau} \mathop{\dd{}\tau},
\end{equation}
whereby we have once again assumed that $I(\tau=\infty)=0$.
If the source function is taken to be constant in each slab this becomes
\begin{equation}\label{Eq:LinearResponse}
    I(\tau=0) = \sum_i S(\tau_i) \int_{\tau_i}^{\tau_{i+1}} e^{-\tau} \mathop{\dd{}\tau},
\end{equation}
and for each slab the associated integral represents its contribution to the outgoing intensity.
In fact, as each contribution is linear in the source function, it represents the \emph{response function} here i.e. the response in the outgoing intensity to a perturbation in the source function at this depth.
We can therefore form a response matrix associating, for each wavelength and depth, the response to a change in source function at this depth.
This response function does not change with the source function due to the linearity of the problem.
In theory, we should therefore be able to infer the depth stratified form of the source function.
However, two primary difficulties arise.

Typically observations of a line contain many more wavelength points than the number of depth points it is feasible to have in our discretised source function, leading to an overdetermined system with no direct inverse.
Even if one were to modify the system so that these dimensions are compatible, the response matrix typically has extremely poor conditioning (is extremely sensitive to errors in the input) and thus cannot be inverted directly.
An immediate solution is then to use a pseudoinverse based on the singular value decomposition of the response matrix, as this can attempt to tackle both problems at once.

Applying this method to a simple example with a quadratically varying source function stratified across 61 depth points using a constant line strength and line profile with 101 wavelength points we find the result shown in Fig.~\ref{Fig:InferredSourceFn}.
The poor quality of the pseudoinverse solution is apparent from the large oscillations and poor agreement with the true source function, and this example showcases the simple case of a static atmosphere with constant line strength.
If these parameters are also allowed to vary then the solution is likely to deteriorate further.

%spell-checker: disable
\begin{pycode}[FlareObs]
import lightweaver as lw
def linear_response_fn(tau, alpha, phi):
    dtau = np.gradient(tau)
    lineDepthIncrease = 1.0 + alpha * phi[None, :]

    responseFn = np.exp(-tau[:, None] * lineDepthIncrease) \
                  * dtau[:, None] * lineDepthIncrease
    return responseFn

Ndepth = 61 # Number of depth points in the stratification
tau = 10**np.linspace(-5, 1, Ndepth) # Optical depth stratification
alpha = 30 # Line strength relative to continuum
x = np.linspace(-20, 20, 101) # Line spectral grid in Doppler units
phi = lw.voigt_H(1.0, x) / np.sqrt(np.pi) # Voigt line profile

# Quadratic Source function peaking at logTau = -1,
# strictly positive and normalised.
S = -(np.log10(tau) + 1)**2
S -= 1.2*S.min()
S /= S.max()

# Compute response function
responseFn = linear_response_fn(tau, alpha, phi)
# Compute outgoing intensity as per equation
Iout = np.sum(S[:, None] * responseFn, axis=0)
# Inferred source function from outgoing intensity and
# response function (via pseudoinverse)
Sinferred = np.linalg.pinv(responseFn.T) @ Iout

fig = plt.figure(figsize=texfigure.figsize(pytex, scale=1))
ax = plt.gca()
plt.plot(np.log10(tau), S, label='True source function')
plt.plot(np.log10(tau), Sinferred, label='Pseudoinverse inferred source function')
plt.xlabel(r'$\log\tau_c$')
plt.ylabel(r'S')
plt.legend(frameon=False, loc='lower right', handlelength=1.5, handletextpad=0.5)

lFig = chFlareObs.save_figure('InferredSourceFn', fig, fext='.pgf')
lFig.caption = r'Comparison between true source function and inferred for the case of a quadratic variation.'
lFig.short_caption = r'Failure of pseudoinverse in inferring quadratic varying source function.'
\end{pycode}
%spell-checker: enable

\py[FlareObs]|chFlareObs.get_figure('InferredSourceFn')|

% Therefore, there is a need for \emph{regularisation} of the solution, penalising deviations from smooth solutions.
% This can be achieved in several ways, a common choice of which is Tikhonov regularisation.
Whilst this problem can be overcome (to some extent) by regularisation of the solution, to enforce smoothness in the ill-posed problem, we are also left with a problem of interpretation for NLTE problems.
Ultimately, we seek to learn information about the atmospheric structure from the outgoing radiation, and not simply the source function.
A natural solution to this is to take a model atmosphere and attempt to fit the synthetic spectrum generated from this model to the observations, but to do this the parameters of the model need to be connected to the synthetic spectrum.

\NOTE{Reorganised the following 2 paragraphs}

It is common to use the integral form of the RTE
\begin{equation}
    I(\nu) = \int_0^\infty S(\nu, t_\nu) e^{-t_\nu} \mathop{\dd{}t_\nu},
\end{equation}
and define the contribution function as the integrand of this expression \citep{Carlsson1997,DelToroIniesta2003}, similarly to the contribution term from \eqref{Eq:LinearResponse}.
This contribution function is often interpreted as a description of the location at which a spectral line forms.
If this can be used to understand how and where the line forms then it seems reasonable to use this as the basis of an inversion scheme.
However, there are several problems that would arise from this approach.
In many lines, the contribution at the line core can span a relatively large region, and thus there is a high probability that photons measured at the same wavelength have been produced in significantly different locations.
It is difficult to meaningfully ascribe mean thermodynamic parameters of line formation to a non-uniform extended region over which the line forms.
This problem becomes more complex still if the contribution function is double-peaked, as is relatively common for lines forming in the complex atmospheres produced by flare models.
As commented by \citet{DelToroIniesta2003} the mathematical definition of the contribution function $C_I$ is also ill-posed, as for any function $f$ such that $\int_0^\infty f(t_\nu) \mathop{\dd{}t_\nu} = 0$, $C_I + f$ is also an equivalently valid contribution function.

The contribution function will qualitatively capture most of the information about the regions important to the formation of a certain line, but it misses the non-local effects that occur in NLTE radiative transfer as the radiation field from one region of the model atmosphere often determines the populations and thus the emissions in another.
These effects can instead be captured by the use of response functions.

Response functions have been primarily used in the field of inversions, but also represent strong tools for understanding the theory of spectral line formation in atmospheric models, due to the close coupling of these two problems.
A response function tells us how the outgoing intensity changes \emph{in response} to a change in an atmospheric parameter.
These were first named by \citet{Beckers1975}, but a similar concept was previously presented by \citet{Mein1971} in the form of weighting functions for the RTE.
They were generalised to full Stokes radiative transfer by \citet{LandiDeglInnocenti1977} and applied by \citet{1992RuizCobo} for the situation of full Stokes synthesis and inversion (using the formulation of \citet{SanchezAlmeida1992}).
A similar approach (built on framing the intensity perturbations as a Fredholm integral equation and using the finite difference method to evaluate necessary terms) for the scalar RTE was employed in the inversions of \citet{Metcalf1990a}, but their focus was on the direct application to inversions rather than the interpretation of the response functions produced.

In the following we shall consider only the response of the unpolarised Stokes I component, as we are focusing on unpolarised models, although the techniques discussed here can easily be applied to the full Stokes case.
The frequency- and depth-dependent response function $\mathcal{R}_{\nu, q} (z)$ to a parameter $q$ can then be defined by
\begin{equation}
    \Delta I(\nu) = \int\mathcal{R}_{\nu, q}(z) \Delta q(z) \mathop{\dd{}z},
\end{equation}
where $\Delta I(\nu)$ is the change in outgoing radiation due to a change of $\Delta q(z)$ in parameter $q$.


To learn from response functions, we first need to know to which parameters we wish to know the intensity response.
This will depend on the parametrisation of the model atmosphere used, but it is common to see response functions to temperature, electron density, velocity, and in the case of spectropolarimetry, magnetic field.
An example of using response functions for chromospheric diagnostics, and their advantages over contribution functions was presented by \citet{Uitenbroek2006}.
It was shown that there can be substantial differences between the contribution and response functions for lines that form in NLTE conditions, and highlighted that for \Ha{} the photospheric conditions will affect the line-core formation height, and hence the source function due to radiative coupling.

In LTE, where the source function is set by the local atmospheric parameters, the response functions can be computed with relative ease.
The SIR inversion code \citep{1992RuizCobo} analytically computes the (full Stokes) response functions to perturbations in different atmospheric parameters at the same time as the formal solution.
These response functions are used in conjunction with a Levenberg-Marquadt damped least squares regression procedure to modify the starting atmosphere (defined on equidistant nodes in $\log \tau$ and assumed to follow cubic splines between nodes) until the synthesised radiation matches the observation as closely as possible.

For lines that are formed well outside LTE the process of determining the response functions to atmospheric perturbations can be significantly more arduous.
The most common approach has been to apply a finite difference method to the outgoing radiation from the statistical equilibrium solution to an atmosphere by successively perturbing each parameter at each node in the atmosphere.
Whilst computationally expensive, this ``brute force'' approach to response functions makes them simple to calculate.
Following this approach, the parameter we wish to know the response to is perturbed at one depth point in our discretised atmosphere, and the populations are updated using the new radiation field and atmosphere.
This procedure is repeated for the parameter at each depth in the atmosphere.
The intensity response to perturbation $\delta q$ in parameter $q$ at depth point $k$ can then be computed by finite differences as
\begin{equation}
    \mathcal{R}_{\nu, q}(k) = \frac{I(\nu, q_k + \delta q) - I(\nu, q_k)}{\delta q}.
\end{equation}
Despite the effective doubling in computational cost, a centred finite difference method was recommended by I. Mili\'{c} (\emph{private communication}) and \citet{DelaCruzRodriguez2017}, and this has proven more robust.
The response function is then written as
\begin{equation}
    \mathcal{R}_{\nu, q}(k) = \frac{I(\nu, q_k + \delta q / 2) - I(\nu, q_k + \delta q / 2)}{\delta q}.
\end{equation}
This process must be undertaken for each depth (or node in the model) and atmospheric parameter independently, leading to its significant computational cost.
In a statistical equilibrium case, the standard procedure of formal solution and population update is repeated until convergence is reached.

Whilst this process is computationally expensive, it has been used reliably since the NICOLE code \citep[developed from \citet{SocasNavarro2000} but no longer using fixed departure coefficients]{Socas-Navarro2015}.
The Stockholm Inversion Code (STiC) also follows this procedure, using a modified form of RH, allowing for the application of PRD \citep{2019dlcr}.

Analytic response functions for the multi-level NLTE problem were first derived by \citet{Milic2017}, and are now implemented in the SNAPI code \citep{Milic2018}.
These response functions should significantly reduce the computational cost of NLTE inversions, a necessity for inverting the large fields of view in current and next generation solar observations.

An alternative approach to reducing the computational overhead of NLTE inversions can be seen in the DeSIRe code, which combines SIR and RH, guiding itself to an approximate solution using the fast analytic LTE response functions of SIR and fine-tuning the solution with finite-difference response functions computed with RH (B. Ruiz Cobo et al. \emph{in preparation}).

All of the codes discussed here use the Levenberg-Marquadt regression method with different varieties of regularisation to enforce smooth solutions.
Additionally, only the statistical equilibrium solution is considered, and then only in hydrostatic equilibrium as this reduces the number of parameters to be inferred.
It is common to allow line-of-sight velocity as a parameter, which technically violates the constraint of hydrostatic equilibrium, however this is a minor effect and is seen as a worthwhile trade-off for the increase in tractability of quiet sun inversions.
Clearly these constraints render this technique very difficult to apply to flares, although NICOLE has been applied to flaring atmospheres by \citet{Kuridze2018}.
The integral approach of \citep{Metcalf1990a} was also applied to flares and differs from the others discussed here, by constructing a matrix of kernels which is solved for temperature and electron density perturbations using a regularised method.
The kernels presented cannot capture the complete response to perturbations in parameters affecting NLTE spectral lines without an approach that can find the new source function (which can be modified non-locally).
These kernels therefore have a narrow range of validity, and require a starting guess close to the true solution.

Response function driven inversions are not the only form of inversions to have been applied, but they are the most widely used.
For example, \citet{AsensioRamos2007} presented a Bayesian approach utilising a Markov Chain Monte Carlo method with Milne-Eddington atmospheres.
This method is too computationally costly to reasonably apply to the full depth-stratified NLTE problem with today's technology, but it would allow the investigation of the posterior distributions of the atmospheric parameters, and be less sensitive to local minima.
With the recent rise in popularity of machine learning, tractable approaches to inversion that allow characterisation of the atmospheric posteriors are a key area of interest \citep[e.g.][]{Osborne2019, DiazBaso2021}, and we will discuss these further, along with other machine learnt approaches in Chap.~\ref{Chap:Radynversion}.


\subsection{Forward Modelling}

A technique related to inversions that has commonly been applied to flares in the last decade is that of forward modelling through the use of RHD codes.
Where possible, the energy input parameters are constrained from observations, via techniques such as X-ray spectroscopy to deduce the non-thermal electron flux and spectral index.
A challenging manual iteration then follows to attempt to obtain an agreement between the time-dependent simulation and the observations.
This is extremely time-consuming both due to the manual aspect of the inversions and the computational requirements of the RHD simulations.
Clearly, the human intervention necessary to optimise and analyse these simulations cannot scale to the large volumes of data coming already present and coming from future telescopes.
This technique has nevertheless yielded many interesting developments in our understanding of the structure of the flaring chromosphere from the investigation of both spectral line shapes and continuum variations \citep{Kuridze2015,RubioDaCosta2016, Kowalski2017,Simoes2017}.

\subsection{In the Context of the Latent Space}

Returning now to the mathematical description of an inversion we can start to discuss the meaning of $\mathcal{Z}$ in practice.
With the response function based inversions described previously, the size of $\mathcal{Z}$ is limited by the constraints placed on the atmospheric stratification, and the regularisation thereof.
It then becomes feasible to ``explore'' this space ($\mathcal{Z}$ coupled with $\mathcal{Y}$ as no explicit distinction is made) using the gradient information from the response functions to guide the solution.
It is worth noting that this approach does not guarantee the global minimum solution; whilst the Levenberg-Marquadt algorithm is extremely efficient at finding local minima, it provides no further guarantees and the final solution may therefore be substantially influenced by the choice of starting atmosphere, which is typically picked based on intuition from the ``standard'' semi-empirical models.
The RHD based forward fitting methods are also comparable in terms of exploration of $\mathcal{Z}$, except here the optimisation is done manually and the gradient information is replaced by intuition.


\section{Introduction to Machine Learning}

Machine learning describes a family of generic algorithms that are used to make sense of data without being explicitly programmed.
A model is defined by the researcher, but its final behaviour is determined by patterns in the data it is fed.
The abundance of both observational and simulated solar data continues to increase and new approaches, such as machine learning, are needed to make use of this vast quantity of information in a computationally tractable manner, helping to highlight patterns that can be further investigated by researchers.

There are three primary varieties of machine learning algorithms: supervised, semi-supervised, and unsupervised learning.
Supervised algorithms are the most common.
The model is provided with a set of examples (typically produced or preprocessed manually) and is then trained so that it represents an approximate transformation between the input and output data defined by the training data.
We can further divide this class into classification and regression models.
Classification associates each class with a discrete input, possibly labelling an image based on its contents, whereas regression approximates a continuous mathematical function.
In both of these cases the model approximates a function which is learnt entirely from the training data.

Unsupervised learning does not require the manually prepared set of examples, but instead organises data based on generic programmed criteria.
Two commonly used examples of unsupervised learning are clustering and dimensionality reduction techniques.
Clustering algorithms extract groups of similar objects (where similar is defined given a particular basis and metric determined by the choice of algorithm), and can be used to find patterns in large datasets.
There are many kinds of dimensionality reduction techniques, but one of the most common and general choices is principal component analysis, where an orthogonal basis spanning the data is constructed and then sorted by the variance of the factors of each of these axes (i.e. the eigenvalues of the covariance matrix).
For data of dimensionality $m$, keeping $m$ principal components allows for a perfect reconstruction, as this is simply a basis transformation, however, we can often discard terms with small variance and produce accurate approximate reconstructions of the data with substantially fewer than $m$ components.
It is necessary to ensure that sufficient components are chosen for the reconstruction to be accurate, but such techniques can reveal patterns that are otherwise difficult to discern in the original high-dimensional spaces.

Finally, as implied by the name, semi-supervised learning lies in between the two previously discussed classes.
It still requires preprocessed training data which is used for some training, however unsupervised learning processes may be used internally to the model, or in some cases data generated by a model is used in conjunction with this training data.
This form of machine learning exists only within the realm of deep learning, built on neural networks.

\subsection{Artificial Neural Networks}

Artificial Neural Networks (ANNs) loosely follow the principle of biological neuronal systems, consisting of layers of interconnected neurons, the output of which is summed in synapses and then has a non-linear activation function applied to determine if the signal is passed on through the network.
ANNs consists of multiple layers of neurons and synapses whereby we designate any layer that is neither the input nor the output a \emph{hidden layer}.
If an ANN consists of more than one hidden layer it is termed a deep neural network (DNN), and these are considered to be the standard building blocks of modern machine learning \citep{Raschka2015}.

There are many different architectures for ANNs, used for solving different problems.
ANNs can vary in number of hidden layers, interconnectedness of neurons within these layers, connectedness of the layers to each other, and the activation function used in each layer.
We distinguish two primary forms of layers, based on their interconnectivity; these are fully connected (FC) where each neuron in a layer is the linear combination of its inputs (typically the activation function applied to the neurons of the previous layer), and the convolutional layers of convolutional neural networks (CNNs, \citet{1998Lecun,2003Simard}) which connect only nearby neurons to exploit local structure in the input (in one or more dimensions).
These convolutional layers can then be described as a set of filters learned during the training process which are cross-correlated with the input, the output of which is then passed through the activation function.
CNNs are somewhat inspired by the neuronal structure of the visual cortex, and have been applied with great success in the fields of image analysis, processing, and generation \citep{Raschka2015}.
A fully connected layer rarely works well for these tasks as a slight movement of an object within an image can easily invalidate its training whereas the smaller layers of a CNN sweep across the image (are applied to each region in turn) and are far less affected by this.

There are many common forms of activation function.
Due to the backpropagation method used for training ANNs it is highly advantageous if these non-linear activation functions be trivially differentiable.
Some common choices are the sigmoid function
\begin{equation}
    \textnormal{S}(x) = \frac{1}{1+e^{-x}},
\end{equation}
inverse tangent $\tan^{-1}(x)$, and variants of the rectified linear unit (ReLU; \citet{2010Nair}).
\begin{equation}
    \textnormal{ReLU}(x) = \textnormal{max}(0, x).
\end{equation}
All of these functions are used in the creation of ANN based models, but the ReLU family is key to modern machine learning for reasons of sparsity in its output.
Here sparsity refers to the presence of zeros in the output of a layer creating clearer pathways through the network.
Classification ANNs will typically employ an activation function on the output layer (most frequently a normalised exponential to select a single discrete class), whereas ANNs employed in regression problems will rarely do so.

\subsection{General Function Approximations}

ANNs are universal function approximators; they can learn arbitrarily complex classification and regression problems \citep{Rumelhart1986,1989Cybenko}.
This was theoretically proven for shallow neural networks (with only one hidden layer) using sigmoidal activation functions by \citet{1989Cybenko}.
Increasing the precision to which a function is approximated may however require exponential increases in layer width and training.
A similar proof for the commonly used ReLU activation function was provided by \citet{Lu2017}, who investigated the bounded expressions for the layer width and network depth needed to approximate functions to arbitrary precision.
Unfortunately these results can be difficult to apply to many real world scenarios where the intrinsic dimensionality of the function being approximated is not known (these results are also affected by any imperfections in the training data).

It is also possible to increase the approximation capability of an ANN by increasing its depth (the number of stacked layers), these stacked layers then represent the composition of functions, and each additional layer increases the complexity of the representation of its input, allowing for very complex tasks to be approximated \citep{Raschka2015}.
The approximation power of stacked layers explains why the DNN is core to modern machine learning, however care must be taken when designing a model to select appropriate width and depth for the problem at hand \citep{Lu2017}.

\subsection{Training via backpropagation}

ANNs are trained via a process known as backpropagation \citep{Rumelhart1986}.
The networks are composed of linear combinations and (by our original requirements) differentiable activation functions.
The entire network can then be differentiated by repeated applications of the chain rule (from output to input) to find the gradients of the output with respect to each weight (the coefficients of the linear combinations in each layer) and input, which then describes how each weight affects the output.
Typically the output of the network when fed with data from the training set is compared against the expected output via a loss function, and then the gradient information from this loss is combined with the previous gradient of the network output to each weight and used to minimise the magnitude of the loss by modifying the weights.

Updating the weights in the network can be carried out in a variety of ways, but it is a similar minimisation process to that used in inversions.
The basic method is that of stochastic gradient descent (SGD) which takes a step through the loss space guided by the gradients for each batch of training data.
The size of this step is known as the learning rate, and is a \emph{hyperparameter}\footnote{Hyperparameters are tunable parameters that are often set by the researcher, or optimised by a process external to the training of the INN} of the ANN.
It can be kept constant, vary following a prescribed evolution with epoch, or even be modified based on the rate of convergence of the training procedure.
As SGD is only affected by the most recent batch of data it can have difficulty escaping local minima and traversing plateaus in the loss space.

Many improvements to SGD have been developed, such as the addition of momentum, which accelerates convergence and helps to avoid the solution being overly affected by a single batch of training data.
Other modern algorithms based on the same principles as SGD have also been developed \citep[e.g. the Adam algorithm,][]{2014Kingma} and often converge in fewer epochs (rounds of training) to similar or better solutions.
None of these stochastic algorithms can guarantee a global minimum in the loss space, and such a requirement is not feasible for anything other than the smallest neural networks, where more time- and memory-consuming optimisers can be used due to the much more dimensionally compact spaces over which the optimisation occurs.
Nevertheless, with sufficient training data and epochs a model capable of approximating the function we wish to learn should be able to descend into sufficiently good local minimum using these techniques.

Auxiliary techniques to improve model convergence have also been developed, such as minibatching, in which the network is only shown a random portion of the training data each epoch.
Clearly this can reduce the computational cost of an epoch, as fewer calculations are performed on this training set, but minibatching can also improve the convergence by avoiding the stagnation that arises in the traditional batched gradient descent where the entire training set is used to direct the step.

A technique known as autodifferentiation has become prominent in the field of machine learning.
It allows users to easily design custom blocks and compose these without the need to consider the implementation of the derivatives needed for training as these are computed by the framework.
Frameworks (e.g. TensorFlow \citep{Abadi2016}, PyTorch \citep{PyTorch}) may record the path of data through a network and then using this information (as every function present therein is differentiable) construct all necessary gradient information for training, which can then be computed on GPU.
The automatic nature of this approach has enabled the rate of development seen in machine learning in the last decade, as it allows researchers to spend longer thinking about design than low-level engineering.

\subsection{Difficulties training DNNs}

As the number of layers in an ANN increases the networks can become much harder to train; the gradient of the output with respect to the weights in early layers can easily become vanishingly small due to the repeated multiplication of small gradients in the deeper layers.
The use of ReLU activation functions often minimises this effect, but can instead lead to exploding gradients due to their high dynamic range.
\citet{2015He} developed residual networks (ResNets), which have greatly increased the depth and complexity of networks that can be effectively trained, and now networks with many hundreds of layers are frequently used \citep[e.g.][]{Jegou2017}.
The residual blocks of these networks contain so-called skip connections, which take the output from a layer and sum or concatenate it with the output of layer one or more levels deeper.
These skip connections provide a path for gradients to propagate through the network, helping to avoid both vanishing and exploding gradients.
Variants of the ReLU function are almost uniquely used in ResNets as these additionally provide sparsity to the representation (i.e. their output is 0 for all input less than or equal to 0), which can improve the expressiveness\footnote{The expressive power of a neural network is its ability to approximate functions.} of the representation and aid in disentangling information propagating through the network \citep{Glorot2011}.
These variants include the leaky ReLU \citep[$\max(0.01x, x)$,][]{Maas2013} which still produces a small amount of gradient information for negative inputs, helping to prevent neurons with ReLU activation from ``dying'', and exponential linear units \citep[ELUs,][]{Clevert2015} which achieve a similar result in a smoothly varying fashion.

Like all regression models with a large number of free parameters, ANNs can very easily enter a regime of overfitting their training set.
In this situation the ANN has learnt to match its training set so closely that it is unlikely to perform reliably on inference of unseen data.
This can often manifest as memorisation, where the network has learnt to produce the expected output for a training sample, but not the relationship between the two.
ANNs must therefore be trained with care and diligent use of validation data, prepared in the same way as the training set, but never shown to the network during training.
The network's performance can be judged by how well it performs in inference on the validation set in between training epochs.
If the performance on the training data continues to improve over time, but the performance on the validation set stagnates or worsens then the network has entered an overfitting regime.

There are additional techniques that can be employed to mitigate overfitting, such as regularisation, which will attempt to prevent a model's weights from minimising the loss function too perfectly, for example by penalising overly large weights with a modified loss function, or randomly deactivating neurons in each layer during training (this approach is known as \emph{dropout}).

Selecting hyperparameters for a model can be a challenging process of manual optimisation but is essential to training, and many advanced optimisers like Adam require additional hyperparameters that can drastically influence the rate of convergence.
Approaches such as grid searches can be applied here, but given the computational requirements of training these models, an intuitive approach is often applied.