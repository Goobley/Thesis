\chapter{RADYNVERSION}\label{Chap:Radynversion}

\begin{pycode}[Radynversion]
name = 'Radynversion'
chRad = texfigure.Manager(
    pytex,
    './04Radynversion',
    number=4,
    python_dir='./04Radynversion/python',
    fig_dir='./04Radynversion/Figs',
    data_dir='./04Radynversion/Data'
)
\end{pycode}

\begin{itemize}
    \item Introduction to inverse problems
    \item Spectral line inversions
    \item Introduction to machine learning
    \item Model description
    \item Model validation
    \item 2014-09-06 flar results
\end{itemize}

\emph{The contents of this chapter are based on my contributions to the research presented in \citet{Osborne2019}.}



\section{Introduction to Inverse Problems}

From the previous chapters we have an understanding of the so-called ``forwards-problem'' of RT, that is to say the problem of going from (in the time-independent case) an atmospheric model to outgoing radiation, and (in the time-dependent case) the atmospheric model and previous populations to the outgoing radiation and new populations.
The inverse of this problem is not well-posed; there is no guarantee of uniqueness, as typically the problem is underdetermined.
The radiation field inside the plasma couples the atomic populations at all depths (as is evident from the form of the $\Lambda$ operator) in a way that cannot be trivially disentangled and information is then lost in the forwards process that is needed for the inverse process.

It is, of course, of great value to constrain the atmospheric parameters associated with a particular observation, and it is this that the field of inversions seeks to achieve, by solving this inverse problem. As observed radiation is the only vector by which information can arrive from the Sun, it is important to maximally exploit the information that can be gleaned from observations. We therefore use inversions to learn as much about the structure of the atmosphere that produced the observed radiation as possible.

% \begin{figure}
% \begin{tikzpicture}[
%     % >=stealth,
%     % bullet/.style={
%     %     fill=black,
%     %     circle,
%     %     minimum width=1pt,
%     %     inner sep=1pt
%     % },
%     % projection/.style={
%     %     ->,
%     %     thick,
%     %     shorten <=2pt,
%     %     shorten >=2pt
%     % },
%     % every fit/.style={
%     %     ellipse,
%     %     draw,
%     %     inner sep=0pt
%     % }
%     % ]
%     \node at (2,4.7) {$f$};
%     % \draw[projection] (1,4.5) -- (3,4.5);
%     \node at (0,5) {$X$};
%     \node[bullet,label=below:$x$]    at (0,2.5){};
%     \node at (4,5) {$Y$};
%     \node[bullet,label=below:$f(x)$] at (4,2.5){};
%     \node at (6,4.7) {$g$};
%     % \draw[projection] (5,4.5) -- (7,4.5);
%     \node at (8,5) {$Z$};
%     \node[bullet,label=below:$g\big(f(x)\big)$] at (8,2.5){};

%     \draw (0,2.5) ellipse (1.02cm and 2.2cm);
%     \draw (4,2.5) ellipse (1.02cm and 2.2cm);
%     \draw (8,2.5) ellipse (1.02cm and 2.2cm);

%     \draw[projection] (0.3,2.5) -- (3.7,2.5);
%     \draw[projection] (4.3,2.5) -- (7.7,2.5);
%     \end{tikzpicture}
% \end{figure}

\begin{figure}
\centering
\begin{tikzpicture}[line width=1pt, >=latex]
    \node (x1) {$T_1,\rho_1,\vec{v}_1, \vec{B}_1$};
    \node[below=of x1] (x2) {$T_2,\rho_2,\vec{v}_2, \vec{B}_2$};
    \node[below=of x2] (x3) {$T_3,\rho_3,\vec{v}_3, \vec{B}_3$};
    \node[below=of x3] (x4) {$T_4,\rho_4,\vec{v}_4, \vec{B}_4$};

    \node[above right=0.5cm and 4cm of x1] (y1) {$\vec{I}_1$};
    \node[below=of y1] (y2) {$\vec{I}_2$};
    \node[below=of y2] (y3) {$\vec{I}_3$};
    \node[below=of y3] (y4) {$\vec{I}_4$};
    \node[below=of y4] (y5) {$\vec{I}_5$};
    % \node[right=4cm of x4] (aux2) {};

    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(x1) (x4)}] (xSet) {};
    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(y1) (y5)}] (ySet) {};

    \node[below=2cm of y5,font=\bfseries, align=center] (yLabel) {$\mathcal{Y}$\\Observed Intensity};
    % \node[left=3cm of ylabel.center,font=\bfseries, align=center] {$\mathcal{X}$\\Atmospheric Parameters};
    \node[font=\bfseries, align=center] (xLabel) at (xSet |- yLabel){$\mathcal{X}$\\Atmospheric Parameters};

    % \draw[->] (xSet.90) to [out=50, in=150] (ySet.90) node[midway] {aa $f(\vec{x})$};
    % \draw[->, dotted] (ySet.270) to [out=210, in=310] (xSet.270) node[midway] {a $f^{-1}(\vec{x})$};
    \draw[->, out=50, in=150] (xSet.90) to node[below] {$f(x)$} (ySet.90);
    \draw[->, dotted, out=210, in=310] (ySet.270) to node[above] {$f^{-1}(y)$} (xSet.270);

    \draw[->] (x1) -- (y2.170);
    \draw[->] (x2) -- (y4.190);
    \draw[->, dotted] (y2.150) -- (x1.5) node[midway, above] {?};
    \draw[->, dotted] (y2.200) -- (x3.20) node[midway, below] {?};
    \draw[->] (x3) -- (y2.175);
    \draw[->] (x4) -- (y5.190);
\end{tikzpicture}
\caption{Degenerate Mapping}
\label{Fig:DegenerateMapping}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}[line width=1pt, >=latex]
    \node (x1) {$T_1,\rho_1,\vec{v}_1, \vec{B}_1$};
    \node[below=of x1] (x2) {$T_2,\rho_2,\vec{v}_2, \vec{B}_2$};
    \node[below=of x2] (x3) {$T_3,\rho_3,\vec{v}_3, \vec{B}_3$};
    \node[below=of x3] (x4) {$T_4,\rho_4,\vec{v}_4, \vec{B}_4$};
    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(x1) (x4)}] (xSet) {};

    \node[above right=1cm and 3.5cm of xSet.center] (y1) {$(\vec{I}_2, z_1)$};
    \node[below=of y1] (y2) {$(\vec{I}_4, z_1)$};
    \node[below=of y2] (y3) {$(\vec{I}_5, z_1)$};
    \node[right=0.5cm of y1] (y4) {$(\vec{I}_2, z_2)$};
    \node[below=of y4] (y5) {$(\vec{I}_4, z_2)$};
    \node[below=of y5] (y6) {$(\vec{I}_5, z_2)$};
    \node[right=0.5cm of y4] (y7) {$(\vec{I}_2, z_3)$};
    \node[below=of y7] (y8) {$(\vec{I}_4, z_3)$};
    \node[below=of y8] (y9) {$(\vec{I}_5, z_3)$};
    % \node[right=4cm of x4] (aux2) {};

    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(y1) (y5) (y9)}] (ySet) {};

    \node[shape=circle, draw=black, right=2cm of y8] (prod) {$\times$};
    \draw[->, ultra thick] (prod.180) -- (ySet.0);

    \node[above=2cm of prod] (yy3) {$\vec{I}_5$};
    \node[above=0.5cm of yy3] (yy2) {$\vec{I}_4$};
    \node[above=0.5cm of yy2] (yy1) {$\vec{I}_2$};
    \node[shape=ellipse,draw=black,minimum size=1cm,fit={(yy1) (yy3)}] (yySet) {};
    \draw[->, ultra thick] (yySet.270) -- (prod.90);

    \node[below=2cm of prod] (z1) {$z_1$};
    \node[below=0.5cm of z1] (z2) {$z_2$};
    \node[below=0.5cm of z2] (z3) {$z_3$};
    \node[shape=ellipse,draw=black,minimum size=1cm,fit={(z1) (z3)}] (zSet) {};
    \draw[->, ultra thick] (zSet.90) -- (prod.270);

    \node[below=6cm of ySet.center,font=\bfseries, align=center] (yLabel) {$\mathcal{Y}\times\mathcal{Z}$\\Observed Intensity $\times$ Latent Space};
    \node[font=\bfseries, align=center] (xLabel) at (xSet |- yLabel){$\mathcal{X}$\\Atmospheric Parameters};
    \node[font=\bfseries, align=center] (zLabel) at (zSet |- yLabel){$\mathcal{Z}$\\Latent Space};
    \node[font=\bfseries, align=center, above=0.2cm of yySet.north] (yyLabel) {$\mathcal{Y}$\\Observed Intensity};

    % \node[below=2cm of y5,font=\bfseries, align=center] (ylabel) {$\mathcal{Y}$\\Observed Intensity};
    % \node[left=3cm of ylabel.center,font=\bfseries, align=center] {$\mathcal{X}$\\Atmospheric Parameters};

    % \draw[->] (xSet.90) to [out=50, in=150] (ySet.90) node[midway] {aa $f(\vec{x})$};
    % \draw[->, dotted] (ySet.270) to [out=210, in=310] (xSet.270) node[midway] {a $f^{-1}(\vec{x})$};
    \draw[->, out=50, in=100] (xSet.90) to node[below] {$g^{-1}(x)$} (ySet.90);
    \draw[->, dotted, out=260, in=310] (ySet.270) to node[above] {$g(y, z)$} (xSet.270);

    \draw[<->] (x1) -- (y4);
    \draw[<->] (x2) -- (y2);
    % \draw[->, dotted] (y2.150) -- (x1.5) node[midway, above] {?};
    % \draw[->, dotted] (y2.200) -- (x3.20) node[midway, below] {?};
    \draw[<->] (x3) -- (y7);
    \draw[<->] (x4.350) -- (y6.190);


\end{tikzpicture}
\caption{Bijective Mapping. For the sake of legibility we have only included the observed intensity vectors which were mapped to in Fig.~\ref{Fig:DegenerateMapping}.}
\label{Fig:BijectiveMapping}
\end{figure}

The forward process can be described by the diagram in Fig.~\ref{Fig:DegenerateMapping}.
This shows the theoretically degenerate nature of the problem; whilst the mapping from $\mathcal{X}$ to $\mathcal{Y}$ is always well-posed, the inverse is not, and with the information present in $\mathcal{Y}$ there is no way to immediately resolve this ambiguity (indicated by the question marks in this figure).
We can instead frame the problem as shown in Fig.~\ref{Fig:BijectiveMapping}, where elements of $\mathcal{Z}$ represent the information lost in the forwards process and a bijective mapping $g$ may be written between these spaces.
Clearly, $\mathcal{Z}$ is difficult to characterise, and we will discuss multiple approaches for replacing or reconstructing this information.
In the following, we denote individual samples of the atmospheric parameters $x$, the emergent line profiles $y$ and the latent space $z$.

\subsection{Milne-Eddington Inversions}

The loss of information can be somewhat limited by placing constraints on the solution. One of the simplest sets of constraints that can be placed on the problem is that of a source function that changes linearly with continuum optical depth, whilst all other parameters are held constant throughout the atmosphere.
This is a low-order approximation of the problem, and is convenient as we can express compute the outgoing intensity analytically.
This can be done for the full Stokes polarised case of the RTE, but for illustration we choose to use only the scalar case here.
With continuum opacity $\tau_c$ the source function is then defined as
\begin{equation}
    S(\tau_c) = S_0 + S_1 \tau_c,
\end{equation}
where $S0$ and $S1$ are constants.
In this situation where we are dealing with the source function directly, rather than atomic parameters (as we are effectively in an two-level atom case), we can define the line strength $\alpha$ as the ratio between the continuum opacity and the line core opacity i.e.
\begin{equation}
    \tau(\nu) = \tau_c (1 + \alpha \phi(\nu)),
\end{equation}
where $\phi$ is the line profile.

Now, by integrating the RTE directly and assuming as semi-infinite atmosphere with no light entering the boundary at $\infty$ we obtain the outgoing intensity
\begin{equation}
I(\tau=0, \nu) = S_0 + \frac{S_1}{1 + \alpha\phi(\nu)}.
\end{equation}

Thanks to the analytic nature of this solution, it is easy to attempt to fit this to observations, and effects such as constant atmospheric velocity, and magnetic field can be included in the line profile.
This approach works quite well for quiet photospheric lines, such as Fe I 6301 \AA{} and Fe I 6173 \AA{} as used in Hinode SOT and HMI \NeedRef{} Centeno?.

The limitations on the description of source function restrict its applicability to chromospheric lines.
However, variants of this approach have implemented more complex shapes for the source function as a function of optical depth, and have had greater success approximating chromospheric lines than the pure linear approximation \NeedRef{} del Toro Iniesta?.

There are substantial limitations to imposing a chosen source function.
A far more flexible approach is to attempt to fit a source function.
From the previous discussion of formal solvers we know that for a discretised atmosphere wherein the source function follows a prescirbed functional form over each interval we can write
\begin{equation}
    I(\tau=0) = \sum_i \int_{\tau_i}^{\tau_{i+1}} S(\tau)e^{-\tau}\, d\tau,
\end{equation}
whereby we have once again assumed that $I(\tau=\infty)=0$.
If the source function is taken to be constant in each slab this becomes
\begin{equation}
    I(\tau=0) = \sum_i S(\tau_i) \int_{\tau_i}^{\tau_{i+1}} e^{-\tau}\, d\tau,
\end{equation}
and for each slab, the associated integral represents its contribution to the outgoing intensity. In fact, as the each contribution is linear in the source function, it represents the \emph{response function} here i.e. the response in the outgoing intensity to a perturbation in the source function at this depth.
We can therefore form a response matrix associating, for each wavelength and depth, the response to a change in source function at this depth.
This reponse function does not change with the source function due to the linearity of the problem.
In theory we should therefore be able to infer the depth stratified form of the source function, however two primary difficulties arise.

Typically observations of a line contain many more wavelength points than the number of depth points it is feasible to have in our discretised source function, leading to an overdetermined system with no direct inverse.
Even if one were to modify the system so that these dimensions are compatible, the response matrix typically has extremely poor conditioning and cannot be inverted directly.
An immediate solution is then to use a pseudoinverse based on the singular value decomposition of the response matrix, as this can tackle both problems at once.
Nevertheless, this tends to produce oscillatory solutions, as shown in Fig. \NeedRef{}.

% Therefore, there is a need for \emph{regularisation} of the solution, penalising deviations from smooth solutions.
% This can be achieved in several ways, a common choice of which is Tikhonov regularisation.
Whilst this problem can be overcome (to some extent) by regularisation of the solution, to enfore smoothness in the ill-posed problem, we are also left with a problem of interpretation for NLTE problems.
Ultimately, we seek to learn information about the atmospheric structure, and not simply the source function.
In LTE, where the source function is set by the local atmospheric parameters, this approach has long been viable.
The SIR inversion code \citep{1992RuizCobo} analytically computes the (full Stokes) response functions (based on the formulations by Sanchez Almeida \NeedRef{}) to perturbations in different atmospheric parameters at the same time as the formal solution.
These response functions are used in conjunction with a Levenberg-Marquadt damped least squares regression procedure to modify the starting atmosphere (defined on equidistant nodes in $\log \tau$ assumed to follow cubic splines between nodes) until the synthesised radiation matches the observation as closely as possible.

For lines that are formed well outside LTE the process of determining the response functions to atmospheric perturbations is significantly more arduous.
The most common approach has been to apply a finite difference method to the outgoing radiation from the statistical equilbrium solution to an atmosphere by successively perturbing each parameter at each node in the atmosphere.
This is incredibly resource intensive, but has reliably been used since the NICOLE code \citep[first distributed 2000]{Socas-Navarro2015}.
The Stockholm Inversion Code (STiC) also follows this procedure, using a modified form of RH, allowing for the application of PRD \citep{2019dlcr}.

Analytic response functions for the multi-level NLTE problem were first derived by \NeedRef{} original \citet{Milic2018}, and are now implemented in the SNAPI code.
These response functions should significantly reduce the computational cost of NLTE inversions, a necessity for current and next generation solar observations.

An alternative approach to reducing the computational overhead of NLTE inversions can be seen in the DeSIRe code, which combines SIR and RH, guiding itself to an approximate solution using the fast analytic LTE response functions of SIR and fine-tuning the solution with finite-difference response functions computed with RH. \NeedRef{}

{\color{Red} HAZEL?}

All of the codes discussed here use the Levenberg-Marquadt regression method with different varieties of regularisation to enforce smooth solutions.
Additionally, only the statistical equilibrium solution is considered, and then only in hydrostatic equilibrium as this reduces the number of parameters to be inferred.
It is common to allow line-of-sight velocity as a parameter, which technically violates the constraint of hydrostatic equilibrium, however this is a minor effect and is seed as a worthwhile trade-off for the increase in tractability of quiet sun inversions.
Clearly these constraints render this technique very difficult to apply to flares, although NICOLE has been applied to flaring atmospheres by \citet{Kuridze2018}.

An ``inversion'' technique that has commonly been applied to flares in the last decade is that of forward modelling through the use of RHD codes.
Where possible the energy input is constrained from observations, via techniques such as X-ray sectroscopy to deduce the non-thermal electron flux and spectral index.
A challenging manual iteration technique then follows to attempt to obtain an agreement between the time-dependent simulation and the observations.
This is extremely time-consuming both due to the manual aspect of the inversions and the computational requirements of the RHD simulations.
Clearly, the human intervention necessary to optimise and analyse these simulations cannot scale to the large volumes of data coming already present and coming from future telescopes.
This technique has nevertheless yielded interesting developments in our understanding of the structure of the flaring chromosphere from the investigation of spectral line shapes and continuum enhancements \citep{Kuridze2015,RubioDaCosta2016, Kowalski2017,Simoes2017}.

Returning now to the lens of our mathematical spaces we can start to discuss the meaning of $z$ in practice.
With the response function based inversions described previously, the size of $z$ is limited by the constraints placed on the atmospheric stratification, and the regularisation thereof.
It then becomes feasible to ``explore'' this space using the gradient information from the response functions to guide the solution.
It is worth noting that this approach does not guarantee the global minimum solution; whilst Levenberg-Marquadt is extremely efficient at finding local minima, it provides no further guarantees and the final solution may therefore be substantially influenced by the choice of starting atmosphere, which is typically made in an ad hoc fashion.
The RHD based methods are also comparable in terms of exploration of $z$, except here the optimisation is done manually and the gradient information is replaced by intuition.

As discussed previously, the assumptions that render NLTE response function based approach tractable, such as hydrostatic equilbrium, cannot be applied in flares where flows close to the sound speed or commonly observed \NeedRef{}.
We therefore need an inversion technique that can operate outside of these constraints.
Our standard RT forwards process can be framed as a function $y = f(x)$ with atmospheric inputs $x$ and line profiles $y$.
Clearly this function is not bijective, but if we also capture the information lost in the forward process, we can instead define a bijective function $x = g(y, z)$ such that $g^{-1}$ represents the forwards process, and $g$ the inverse process.
Our theory of radiative transfer does not give any immediate insight into the formulation of $g$ with so few constraints, so we instead turn to the field of machine learning.

\section{Introduction to Machine Learning}

Machine learning describes a family of generic algorithms that are used to make sense of data without being explicitly programmed.
A model is defined by the researcher, but its final behaviour is determined by patterns in the data it is fed.
The abundance of both observational and simulational solar data continues to increase and new approaches, such as machine learning, are needed to make use of this vast quantity of information in a computationally tractable manner, helping to highlight patters that can be further investigated by researchers.

There are three primary varieties of machine learning algorithms: supervised, semi-supervised, and unsupervised learning.
Supervised algorithms are the most common.
The model is provided with a set of of examples (typically produced or preprocesed manually) and is then trained so that it represents and approximate transformation between the input and output data defined by the training data.
We can further divide this class into classification and regression models.
Classification associates each class with a discrete input, possibly labelling an image based on its contents, whereas regression approximates a continuous mathematical function.
In both of these cases the model approximates a function which is learnt entirely from the training data.

Unsupervised learning does not require the manually prepared set of examples, but instead organises data based on generic programmed criteria.
Two commonly used examples of unsupervised learning are clustering and dimensionality reduction techniques.
Clustering algorithms extract groups of similar objects (where similar is defined given a particular basis and metric determined by the choice of algorithm), and can be used to find patterns in large datasets.
There are many kinds of dimensionality reduction techniques, but one of the most common and general choices is principal component analysis, where an orthogonal basis spanning the data is constructed and then sorted by the variance of the factors of each of these axes (i.e. the eigenvalues of the covariance matrix \NeedRef{}).
For data of dimensionality $m$, keeping $m$ principal components allows for a perfect reconstruction, as this is simply a basis transformation, however, we can often discard terms with small variance and produce accurate approximate reconstructions of the data with substantially fewer than $m$ components.
It is necessary to ensure that sufficient components are chosen for the reconstruction to be accurate, but such techniques can reveal patterns that are otherwise difficult to discern in the original high-dimensional spaces.

Finally, as implied by the name, semi-supervised learning lies in between the two previously discussed classes.
It still requires preprocessed training data which is used for some training, however unsupervised learning processes maybe used internally to the model, or in some cases data generated by the model is used in conjunction with this training data.
This form of machine learning exists only within the realm of deep learning, built on neural networks.

\subsection{Artificial Neural Networks}

Artificial Neural Networks (ANNs) loosely follow the principle of biological neuronal systems, consisting of layers of interconnected neurons, the output of which is summed in synapses and then has a non-linear activation function applied to determine if the signal is passed on through the network.
ANNs consists of multiple layers of neurons and synapses whereby we designate any layer that is neither the input nor the output a \emph{hidden layer}.
If an ANN consists of more than one hidden layer it is termed a deep neural network (DNN), and these are considered to be the standard building blocks of modern machine learning \citep{Raschka2015}.

There are many different architectures for ANNs, used for solving different problems.
ANNs can vary in number of hidden layers, interconnectedness of neurons within these layers, connectedness of the layers to each other, and the activation function used in each layer.
We distinguish two primary forms of layers, based on their interconnectivity, these are fully connected (FC) where each neuron in a layer is the linear combination of its inputs (typically the activation function applied to the neurons of the previous layer), and the convolutional layers of convolutional neural networks (CNNs, \citet{1998Lecun,2003Simard}) which connect only nearby neurons to exploit local structure in the input (in one or more dimensions).
These convolutional layers can then be described as a set of filters learned during the training process which are cross-correlated with the input, the output of which is then passed through the activation function.
CNNs are somewhat inspired by the neuronal structure of the visual cortex, and have been applied with great success in the fields of image analysis, processing, and generation \citep{Raschka2015}.
A fully connected layer rarely works well for these tasks as a slight movement of an object within an image can easily invalidate its training whereas the layers of a CNN sweep across the image and are far less affected by this.

As previously mentioned there are many common forms of activation function.
Due to the backpropagation method used for training ANNs it is highly advantageous if these non-linear activation functions be trivially differentiable.
Some common choices are the sigmoid function
\begin{equation}
    \mathrm{S}(x) = \frac{1}{1+e^{-x}},
\end{equation}
inverse tangent $\tan^{-1}(x)$, and variants of the rectified linear unit (ReLU; \citet{2010Nair}).
\begin{equation}
    \mathrm{ReLU}(x) = \mathrm{max}(0, x).
\end{equation}
All of these functions are used in the creation of ANN based models, but the ReLU family is key to modern machine learning for reasons of sparsity in its output.
Classification ANNs will typically employ an activation function on the output layer (most frequently a normalised exponential to select a single discrete class), whereas ANNs employed in regression problems will rarely do so.

\subsection{General Function Approximations}

ANNs are universal function approximators; they can learn arbitrarily complex classification and regression problems \citep{Rumelhart1986,1989Cybenko}.
This was theoretically proven for shallow neural networks (with only one hidden layer) using sigmoidal activation functions by \citet{1989Cybenko}.
Increasing the precision to which a function is approximated may however require exponential increases in layer width and training.
A similar proof for the commonly used ReLU activation function was provided by \citet{Lu2017}, who also investigated the width needed to approximate different functions.
Unfortunately these results can be difficult to apply to many real world scenarios where the intrinsic dimensionality of the function being approximated is not known (these results are also affected by any imperfections in the training data).
It is also possible to increase the approximation capability of an ANN by increasing its depth (the number of stacked layers), these stacked layers then represent the composition of functions, and each additional layer increases the complexity of the representation of its input, allowing for very complex tasks to be approximated \citep{Raschka2015}.
The approximation power of stacked layers explains why the DNN is core to modern machine learning, however care must be taken when designing a model to select appropriate width and depth for the problem at hand \citep{Lu2017}.

\subsection{Training via backpropagation}

ANNs are trained via a process known as backpropagation \citep{Rumelhart1986}.
The networks are composed of linear combinations and (by our original requirements) differentiable activation functions.
The entire network can then be differentiated by repeated applications of the chain rule (from output to input) to find the gradients of the output with respect to each weight and input, which then describes how each weight affects the output.
Typically the output of the network when fed with data from the training set is compared against the expected output via a loss function, and then the gradient information from this loss used to minimise its magnitude.

Updating the weights (the coefficients of the linear combinations in each layer) in the network can be carried out in a variety of ways, but it is a similar minimisation process to that used in inversions.
The basic method is that of stochastic gradient descent (SGD) which takes a step through the loss space guided by the gradients for each batch of training data.
The size of this step is known as the learning rate, and is a \emph{hyperparameter}\footnote{Hyperparameters are tunable parameters that are often set by the researcher, or optimised by a process external to the training of the INN} of the ANN.
It can be kept constant, vary following a prescribed evolution with epoch, or even modified based on the rate of convergence of the traioning procedure.
As SGD is only affected by the most recent batch of data it can have difficulty escaping local minima and traversing plateaus in the loss space.

Many improvements to SGD have been developed, such as the addition of momentum, which accelerates convergence and helps to avoid the solution being overly affected by a single batch of training data.
Other modern algorithms based on the same principles as SGD have also been developed (e.g. Adam \citet{2014Kingma}) and often converge in fewer epochs (rounds of training) to similar or better solutions.
None of these stochastic algorithms can guarantee a global minimum in the loss space, and such a requirement is not feasible for anything other than the smallest neural networks, where more time- and memory-consuming optimisers can be used due to the much more dimensionally compact spaces over which the optimsation occurs.
Nevertheless, with sufficient training data and epochs a model capable of approximating the function we wish to learn should be able to descend into sufficiently good local minimum using these techniques.

Auxiliary techniques to improve model convergence have also been developed, such as minibatching, in which the network is only shown a random portion of the training data each epoch.
Clearly this can reduce the computational cost of an epoch, as fewer calculations are performed on this training set, but minibatching can also improve the convergence by avoiding the stagnation that arises in the traditional batched gradient descent where the entire training set is used to direct the step.

A technique known as autodifferentiation has become prominent in the field of machine learning.
It allows users to easily design custom blocks and compose these without the need to consider the implementation of the derivatives needed for training as these are computed by the framework.
Frameworks (e.g. TensorFlow, PyTorch \NeedRef{}) may record the path of data through a network and then using this information (as every function present therein is differentiable) construct all necessary gradient information for training, which can then be computed on GPU.
This automatic nature of this approach has enabled the rate of development seen in machine learning in the last decade, as it allows researchers to spend longer thinking about design than low-level engineering.

\subsection{Difficulties training DNNs}

As the number of layers in an ANN increase they can become much harder to train; the gradient with respect to the weights in early layers can easily become vanishingly small due to the repeated multiplication of small gradients in the deeper layers.
The use of ReLU activation functions often minimises this effect, but can instead lead to exploding gradients due to their high dynamic range.
\citet{2015He} developed residual networks (ResNets), which have greatly increased the depth and complexity of networks that can be effectively trained, and now networks with many hundreds of layers are frequently used \citep{Jegou2017}.
The residual blocks of these networks contain so-called skip connections, which take the output from a layer and sum or concatenate it with the output of layer one or more levels deeper.
These skip connections provide a path for gradients to propagate through the network, helping to avoid both vanishing and exploding gradients.
Variants of the ReLU function are almost uniquely used in ResNets as these additionally provide sparsity to the representation (i.e. their output is 0 for all input less than or equal to 0), which can improve the expressiveness of the representation and aid in disentangling information propagating through the network \citep{Glorot2011}.
These variants include the leaky ReLU \citep[$\max(0.01x, x)$;][]{Maas2013} which still produces a small amount of gradient information for negative inputs, helping to prevent neurons with ReLU activation from ``dying'', and exponential linear units \citep[ELUs;][]{Clevert2015} which achieve a similar result in a smoothly varying fashion.

Like all regression models with a large number of free parameters, ANNs can very easily enter a regime of overfitting their training set.
In this situation the ANN has learnt to match its training set so closely that it is unlikely to perform reliably on inference of unseen data.
Oftentimes this can manifest as memorisation, where the network has learnt to produce the expected output for a training sample, but not the relationship between the two.
ANNs must therefore be trained with care and diligent use of validation data, prepared in the same way as the training set, but never shown to the network during training.
The network's performance can be judged by how well it performs in inference on the validation set in between training epochs.
If the performance on the training data continues to improve over time, but the performance on the validation set stagnates or worsens then the network has entered an overfitting regime.

There are additional techniques that can be employed to mitigate overfitting, such as regularisation, which will attempt to prevent a model's weights from minimising the loss function too perfectly, for example by penalising overly large weights with a modified loss function, or randomly disactivating neurons in each layer during training (this approach is known as \emph{dropout}).

The process of selecting hyperparameters for a model can be a challenging process of manual optimisation that is essential to training, and many advanced optimisers like Adam require additional hyperparameters that can drastically influence the rate of convergence.
Approaches such as grid searches can be applied here, but given the computational requirements of training these models, an intuitive approach is often applied.

\section{The RADYNVERSION Model}\label{Sec:RadynversionModel}

Our approach towards inversions of solar flares is based upon the application of machine learning to the problem as it is framed in Fig.~\ref{Fig:BijectiveMapping}.
We wish to learn the form of the latent space, and by sampling this space sufficiently for a given observation we can infer the likelihoods of parameters of the flaring atmosphere.
Nevertheless, we have no data on which to train which directly characterises the latent space, amd thus we turn to the technique of invertible neural networks (INNs) which naturally learn bijective functions to learn the bijective mapping \emph{and} the form of the latent space simultaneously.

Our INN is trained using RHD simulations generated with \Radyn{}, which contain the structure of a model flaring atmosphere and the synthesised emergent radiation (here the \Ha{} and \CaLine{} spectral lines).
As the function $x = g(y, z)$ (as shown in Fig.~\ref{Fig:BijectiveMapping}) is bijective, our INN can learn the function $g^{-1}$ as the forwards process, and $g$ as the inverse process.
ANNs can learn to approximate any function, so they can be trained to transform any distribution to any other.
We can therefore choose to characterise the latent space as any smooth continuous distribution desired and the INN will internally learn the mapping to the form of the ``true'' latent space contained within the simulations.
The approach to inferring atmospheric parameters using the INN therefore differs to the approach of traditional methods that take guided explorations of the latent space.
Due to the speed of the INN solution relative to the cost of formal solution and iteration that is required in conventional NLTE inversions, we can simply take a large number of samplings of our chosen latent space, and thus generate a probability density function for the atmospheric parameters at each location in the atmosphere, conditioned by our training set.
For simplicity we choose to represent the latent space as the unit multivariate Gaussian distribution with mean 0 and variance 1, which we denote $\mathcal{N}(0, \mathcal{I}_n)$ for an $n$ dimensional case.

The INN is a form of DNN in which invertible blocks are used.
A traditional fully connected or convolutional layer is not generally invertible.
Indeed, whilst a fully connected layer with equal number of inputs and outputs, and an invertible activation function \emph{can} be inverted, it is extremely computationally expensive to do so due to the cost of inverting the potentially large square matrix of weights.
Instead, we use \emph{affine coupling layers} \citep{2014Dinh,2016Dinh} which are trivially reversible, and thus during the training of the forward process the inverse of this learned function is simultaneously trained ``for free''.
The affine coupling layers used in our INN were first presented by \citet{2018Ardizzone}.
The input vector $\vec{x}$ is first split into two halves $[x_1, x_2]$ which undergo the following affine transformations
\begin{align}
    y_1 &= x_1 \otimes \exp(s_2(x_2)) + t_2(x_2),\\
    y_2 &= x_2 \otimes \exp(s_1(y_1)) + t_1(y_1),
\end{align}
where $\otimes$ represents the elementwise product of tensors and $s_i$ and $t_i$ ($i \in \{1, 2\}$) are arbitrarily complex differentiable functions (that need not be invertible).
It is worth noting the order of operations here, as $y_1$ must be computed before $y_2$.
The output $\vec{y}$ is then constructed from the concatenation of $y_1$ and $y_2$.
The inverse of these affine transforms is given by
\begin{align}
    x_2 &= (y_2 - t_1(y_1)) \oslash \exp(s_1(y_1)),\\
    x_1 &= (y_1 - t_2(x_2)) \oslash \exp(s_2(x_2)),
\end{align}
where $\oslash$ represents the elementwise division of tensors.
We have stated that the $s_i$ and $t_i$ functions can be of arbitrary complexity, but clearly they need to be tailored to the particular task and for this reason we apply DNNs in this role.
The networks used for $s_i$ and $t_i$ are identical, save for the application of a $\tan^{-1}$ transformation on the output of the $s_i$ block, which prevents extreme values from being produced whereby the exponential term would domination, or have no effect on the output of the block.

In the spirit of DNNs we then stack multiple affine coupling layers to allow for increased representational capability within the INN.
As the the input is split in half upon entering each affine layer, we can see that the data in each half is only combined at the elementwise multiplication step in each layer on its journey through the network.
To alleviate this, and further increase the generalisation capabilities of the network, we interleave a permutation layer between each affine coupling layer.
This layer shuffles the data in a random, but fixed order which is different for each permuation layer, but allows for trivial reversibility.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[line width=1pt, >=latex,
    block/.style={
    draw,
    rectangle,
    minimum width={width("Affine Coupling Layer")+2pt},
    font=\small}]
    \node [draw, anchor=center] (Input) {Input};

    \node [block, right=1cm of Input.0, rotate=90, anchor=north] (Acl1) {Affine Coupling Layer};
    \node [block, right=of Acl1.north, rotate=90, anchor=north] (P1) {Permutation Layer};

    \node [block, right=of P1.north, rotate=90, anchor=north] (Acl2) {Affine Coupling Layer};
    \node [block, right=of Acl2.north, rotate=90, anchor=north] (P2) {Permutation Layer};

    \node [block, right=of P2.north, rotate=90, anchor=north] (Acl3) {Affine Coupling Layer};
    \node [block, right=of Acl3.north, rotate=90, anchor=north] (P3) {Permutation Layer};

    \node [block, right=of P3.north, rotate=90, anchor=north] (Acl4) {Affine Coupling Layer};
    \node [block, right=of Acl4.north, rotate=90, anchor=north] (P4) {Permutation Layer};

    \node [block, right=of P4.north, rotate=90, anchor=north] (Acl5) {Affine Coupling Layer};

    \node [draw, below right=1cm and 1cm of Acl5.center, anchor=180] (LatentSpace) {Latent Space};
    \node [draw,
           minimum width={width("Latent Space")+2pt},
           above right=1cm and 1cm of Acl5.center, anchor=180] (Output) {Output};

    \draw[->, color=TolBlue] (Input.20) to (Input.20 -| Acl1.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl1.south) -- (Input.20 -| P1.north);
    \draw[->, color=TolBlue] (Input.20 -| P1.south)   -- (Input.20 -| Acl2.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl2.south) -- (Input.20 -| P2.north);
    \draw[->, color=TolBlue] (Input.20 -| P2.south)   -- (Input.20 -| Acl3.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl3.south) -- (Input.20 -| P3.north);
    \draw[->, color=TolBlue] (Input.20 -| P3.south)   -- (Input.20 -| Acl4.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl4.south) -- (Input.20 -| P4.north);
    \draw[->, color=TolBlue] (Input.20 -| P4.south)   -- (Input.20 -| Acl5.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl5.south) to[out=0, in=180] (Output.180);

    \node [draw, circle, fill, inner sep=0pt, minimum size=2pt,
           right=2cm of Acl5.center] (Mix) {};

    \draw[->, color=TolTeal] (Output.south -| Mix) -- (Mix);
    \draw[->, color=TolTeal] (LatentSpace.north -| Mix) -- (Mix);
    \draw[->, color=TolTeal] (Mix) to[out=180, in=0] (Input.340 -| Acl5.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl5.north) -- (Input.340 -| P4.south);
    \draw[->, color=TolTeal] (Input.340 -| P4.north) -- (Input.340 -| Acl4.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl4.north) -- (Input.340 -| P3.south);
    \draw[->, color=TolTeal] (Input.340 -| P3.north) -- (Input.340 -| Acl3.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl3.north) -- (Input.340 -| P2.south);
    \draw[->, color=TolTeal] (Input.340 -| P2.north) -- (Input.340 -| Acl2.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl2.north) -- (Input.340 -| P1.south);
    \draw[->, color=TolTeal] (Input.340 -| P1.north) -- (Input.340 -| Acl1.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl1.north) -- (Input.340);
\end{tikzpicture}
\caption{Structure of the final RADYNVERSION network. The forwards process is shown with the {\color{TolBlue}blue} arrows, and the inverse process with {\color{TolTeal}teal} arrows.}
\label{Fig:RadynversionDiagram}
\end{figure}

For the RADYNVERSION\footnote{RADYNVERSION is a portmanteau of \Radyn{} and ``inversion''.} model we use five affine coupling layers, with four interleaved permutation layers.
This is shown schematically in Fig. \ref{Fig:RadynversionDiagram}.
Given the presence of four DNNs per affine coupling layer, our final model is composed of twenty DNNs.
Each of these networks is an individual four-layer fully connected network utilising leaky ReLU activation functions after each of the first three layers, and a ReLU following the final layer.

The input to the RADYNVERSION network then consists of a reduced set of atmospheric parameters at a given point in time, in our case the temperature $T$, electron density $n_e$, and velocity $v$.
These are provided to the network on a fixed height stratification, with 50 points covering the entire atmosphere.
The arge dynamic range of these parameters (both across a single atmosphere, and between different timesteps at a fixed location in the atmosphere) can have a negative impact on the training and accuracy of the ANN, and we therefore map $T\mapsto\log_{10} T$, $n_e\mapsto\log_{10} n_e$, and $v\mapsto\mathrm{sign}(v)\log_{10} \left( |v_{\mathrm{km\, s^{-1}}}| + 1\right)$.
The mapping for $v$ serves to scale it based on its base-10 logarithmic value, whilst preserving its sign information.
These logarithmic mappings preserve detail better over each decade than a simple linear rescaling.

The output consists of line profiles, in this case \Ha{} and \CaLine{}.
Both of these are interpolated onto fixed wavelength grids, with 30 points each (a half width of \SI{0.14}{\nano\metre} for \Ha{} and \SI{0.1}{\nano\metre} for \CaLine{}).
The intensity values of the spectral lines are scaled to cover the range [0, 1] whilst preserving the relative intensity of the two (which conveys important information regarding continuum emission).

Our model then has an input dimensionality of 150 and an output dimensionality of 60.
We choose, by experimentation, to set the size of the latent space to same as the input, however we cannot prove the opimality of such a choice as it depends on the (unknown) intrinsic dimensionality of the problem.
Thus the input and total output (i.e. output and latent space) must be of length at least 210.
To improve the generalisation performance of the network and allow it a greater dimensionality for its representation of the data we choose to set the input and output size to 384.
The input to the network is zero-padded to this length, and the output and latent space are concatenated with zero-padding in between.
The deep neural networks used inside the affine coupling layers have an input and output of length 192, but this is increased to 384 for the inner layers, to further increase their representational ability.

The RADYNVERSION model presented here considers each set of atmospheric parameters and observables as instantaneous quantities.
As we have discussed, it is usually necessary to consider time-dependent populations in flares, and clearly we do not do that here.
The atomic populations are not considered directly with this model, and we are interested in whether there is sufficient information present in this reduced description of the atmosphere to reproduce the emergent line profiles and whether an ANN can then learn to decode this information.

\subsection{Training Data}

To train this model we use 81 \Radyn{} simulations computed by the F-CHROMA project \NeedRef{}.
These models all start from a variant of the VAL3C quiet sun atmosphere \citep{Vernazza1981}, slightly modified to remain stable in \Radyn{}.
All models are heated by a symmetric triangular electron beam pulse, modelled using the Fokker-Planck module (with an initial power law distribution of electron energies), of \SI{20}{\s} duration, with a peak at \SI{10}{\s}.
The total beam deposition varies between \SI{3e10}{\erg\per\square\cm} and \SI{1e12}{\erg\per\square\cm}, the low-energy cutoff is one of \{10, 15, 20, 25\}~\si{\kilo\electronvolt}, and the spectral index of the electron energy distribution is one of \{3, 4, 5, 6, 7, 8\}.
All of these simulations last for \SI{50}{\s}, with data saved every \SI{0.1}{\s}.

Not all combinations of these parameters converged, in particular some simulations with lower values for the low-energy cutoff, higher spectral indices, and high total energy deposition were not present in the grid.
From the 81 simulations we then have 40,500 individual timesteps of which we separate 20\% for validation purposes.
The atmospheric parameters and line profiles are mapped onto their fixed grids and prepared as discussed in Section~\ref{Sec:RadynversionModel}.
Our height stratification is chosen to primarily sample the chromosphere, and places 45 linearly spaced points below \SI{3.5}{\mega\metre}, with a constant spacing of \SI{79.2}{\kilo\metre}.
The remaining 5 points are then expontially spread through the corona from \SIrange{3.5}{10}{\mega\metre}.

\subsection{Training Method}

Our training method is based on the one presented in \citet{2018Ardizzone} and makes the network is constructed using their framework. \NeedRef{}
The INN is trained in both directions to ensure the conditioning of both the forwards and inverse problems.
The model is trained using minibatching with the same minibatch of the training set being used in both directions.
Both training directions are also constrained by two loss functions.
The forwards direction (from atmospheric parameters to line profiles) uses an L2 loss ($||y-y_\mathrm{true}||_2^2$, where $y$ indicates the output of the network and $y_\mathrm{true}$ the expected output) on the output vector of line profiles and zero padding.
This latent space is constrained by a Maximum Mean Discrepancy (MMD) loss.
The MMD is a loss that compares distributions from finite samples, and is computed between batches of [$y$, $z$] and [$y_\mathrm{true}$, $\mathcal{N}(0, \mathcal{I}_z)$].
This is discussed in depth, along with implementation details in Section~\ref{Sec:Mmd}.
During the forward process the MMD loss is used to ensure that the network learns to map the true latent space to our chosen form for it (the multivariate unit Gaussian distribution).
A traditional regression loss cannot be applied here, as we would have to assign fixed ``samples'' from the latent space for each timestep in the training set, which cannot be done without understanding the true latent space.
Its aim is instead to condition the distribution produced in the latent space.
To this end, whilst $y$ is included in the MMD loss terms (as this is an important component of the output), the gradients on $y$ due to the MMD loss are ignored, so as not to affect the training of the forwards model.
For both the L2 and MMD losses to converge on the forwards process ensures that samples of $z$ are correctly independent of $y$ as they must not contain copies of the same information for the reverse process to work correctly.

The reverse process is trained similarly, with an additional two losses.
An L2 loss is used for $x$ and the zero-padding to ensure the expected atmosphere parameters are produced (and that the padding remain 0), and an MMD loss ensuring the correct distribution of $x$ for random latent samples.

Both of the forwards and backwards losses are linearly combined to produce a set of weighted gradients used to update the network.
We define three hyperparameter weights for this purpose $w_\mathrm{pred}$, $w_\mathrm{latent}$, and $w_\mathrm{rev}$.
These are combined to produce the losses
\begin{align}
    loss_f &= w_\mathrm{pred} L2_f + w_\mathrm{latent} MMD_f,\\
    loss_b &= 0.5w_\mathrm{pred} L2_b + \xi(n)w_\mathrm{rev} MMD_b,
\end{align}
where $f$ and $b$ represent the forwards and backwards terms respectively, and $\xi$ is a term that gradually increases towards unity so as to limit the impact of the backwards MMD term on early epochs.
It is parameterised as
\begin{equation}
    \xi(n) = \left( \min\left( \frac{n}{0.4 N_\mathrm{fade}}, 1 \right) \right)^3,
\end{equation}
where $n$ is the current epoch, and $N_fade$ is the number of epochs needed for this term to become unity.
To ensure that the output in the zero-padded sections remains close to zero we use $xi$ to initialise these to a small amount of random noise, decaying over this same period.
This increases the activation of these neurons early on, forcing the network to learn that these must be adjusted towards 0 for all inputs.
Empirically we found that for the fade-in period of 800 epochs (over which $\xi$ increases to 1), a good choice for the loss weights was $w_\mathrm{pred}=$4000, $w_\mathrm{latent}=$900, and $w_\mathrm{rev}=$1000.
After the fade-in period the network was trained in blocks of 400 epochs, increasing the value of $w_\mathrm{pred}$ by 1000 for each block of these.
From 4,800 epochs to 12,000 epochs the network was trained in blocks of 600 epochs, with the value of $w_\mathrm{pred}$ being increased by the same amount.
These weights were all tuned empirically, and others were also found to yield good convergence, however we found it important that the L2 weight be a factor of 2 or more larger than the MMD weights or the forwards process would not reliably converge.

The gradients computed from these losses are used in conjunction with the Adam optimiser \citep{2014Kingma} with $\beta_1=\beta_2=0.8$ and $\epsilon=1\times10^{-6}$.
The $\beta_i$ terms control the decay rate for momentum of the first- and second-moment estimates of the gradients and $\epsilon$ simply prevents division by zero.
These gradients are clipped to a range of $\pm15$ to help further mitigate problems with exploding gradients.
This does not affect the final solution as the gradients will decrease as we approach a minimum.
The learning rate was initialised to $1.5\times10^{-3}$ and decays by a factor of $0.004^{1/1333}$ every 12 epochs.
Each minibatch contained 500 different samples and 20 minibatches were used per learning epoch.
The final model was selected based on its L2 performance for the forwards and backwards results on the validation set.
In this case the best performing model was the one saved after 11,400 epochs of training.



\subsection{Maximum Mean Discrepancy}\label{Sec:Mmd}

\subsection{Validation}

\begin{pycode}[Radynversion]
def logvel_to_vel(v):
    vSign = v / np.abs(v)
    vSign[np.isnan(vSign)] = 0
    vel = vSign * (10**np.abs(v) - 1.0)
    return vel

def find_range_in_schema(schema, name):
    nameIdx = 0
    for i, s in enumerate(schema):
        if s[0] == name:
            nameIdx = i
            break
    else:
        raise ValueError('Name %s not found in schema %s' % (name, repr(schema)))

    startLoc = 0
    for i in range(nameIdx):
        startLoc += schema[i][1]
    endLoc = startLoc + schema[nameIdx][1]
    return range(startLoc, endLoc)

with open(chRad.data_file('Validation/Schemas.pickle'), 'rb') as f:
    schemas = pickle.load(f)

forwards = np.load(chRad.data_file('Validation/ForwardsProcess.npz'))
reverse = np.load(chRad.data_file('Validation/ReverseProcess.npz'))
z = schemas['z']
zMm = schemas['z'] / 1e8
wls = schemas['wls']

fig, ax = plt.subplots(2, 2, #constrained_layout=True,
                       figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.8))
ax = ax.ravel()
ax = [ax[0], ax[0].twinx(), *ax[1:]]
ax[0].plot(zMm, forwards['xTrue'][0, 0])
ax[1].plot(zMm, forwards['xTrue'][0, 1], c='C1')
ax[2].plot(zMm, logvel_to_vel(forwards['xTrue'][0, 2]), c='C2')
ax[0].set_ylabel(r'$\log{n_e}$ [\si{\cm\tothe{-3}}]', c='C0')
ax[0].set_xlabel(r'$z$ [\si{\mega\metre}]')
ax[1].set_ylabel(r'$\log{T}$ [\si{\K}]', c='C1')
ax[2].set_ylabel(r'$v$ [\si{\kilo\metre\per\second}]', c='C2')
ax[2].set_xlabel(r'$z$ [\si{\mega\metre}]')


halphaCoords = find_range_in_schema(schemas['outSchema'], 'Halpha')
caCoords = find_range_in_schema(schemas['outSchema'], 'Ca8542')
haCentre = 0.5 * (wls[0][0] + wls[0][-1])
caCentre = 0.5 * (wls[1][0] + wls[1][-1])
ax[3].plot((wls[0] - haCentre) / 10, forwards['yTrue'][0, 0], c='C2')
ax[3].plot((wls[0] - haCentre) / 10, forwards['yzPred'][0, halphaCoords], '-.', c='C4')
ax[4].plot((wls[1] - caCentre) / 10, forwards['yTrue'][0, 1], c='C2', label='Ground Truth')
ax[4].plot((wls[1] - caCentre) / 10, forwards['yzPred'][0, caCoords], '-.', c='C4', label='Predicted')
ax[3].xaxis.set_major_locator(plt.MaxNLocator(6))
ax[4].xaxis.set_major_locator(plt.MaxNLocator(6))
fig.legend(loc='lower left', frameon=False, bbox_to_anchor=(0.42, 0.45))

ax[3].set_ylabel('Normalised Intensity')
ax[3].set_xlabel(r'$\Delta\lambda$ [\si{\nano\metre}]')
ax[4].set_xlabel(r'$\Delta\lambda$ [\si{\nano\metre}]')
ax[3].set_title(r'H$\alpha$')
ax[4].set_title(r'Ca\,\textsc{ii} \SI{854.2}{\nano\metre}')

fig.tight_layout()
latexFig = chRad.save_figure('RadynversionValidationForwards', fig, fext='.pgf')
latexFig.caption = 'Validation of the Radynversion Forwards Process'
\end{pycode}


\py[Radynversion]|chRad.get_figure('RadynversionValidationForwards')|

\begin{pycode}[Radynversion]
from matplotlib.colors import LinearSegmentedColormap, PowerNorm
cmapNe = [(1.0,1.0,1.0,0.0), (51/255, 187/255, 238/255, 1.0)]
cmapNe = [(1.0,1.0,1.0,0.0), (*sns.color_palette()[0], 1.0)]
neColors = LinearSegmentedColormap.from_list('ne', cmapNe)
cmapTemp = [(1.0,1.0,1.0,0.0), (238/255, 119/255, 51/255, 1.0)]
cmapTemp = [(1.0,1.0,1.0,0.0), (*sns.color_palette()[1], 1.0)]
tempColors = LinearSegmentedColormap.from_list('temp', cmapTemp)
cmapVel = [(1.0,1.0,1.0,0.0), (238/255, 51/255, 119/255, 1.0)]
cmapVel = [(1.0,1.0,1.0,0.0), (*sns.color_palette()[2], 1.0)]
velColors = LinearSegmentedColormap.from_list('vel', cmapVel)
powerNormIdx = 0.3

fig, ax = plt.subplots(1, 2, #constrained_layout=True,
                       figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.4))
ax = ax.ravel()
ax = [ax[0], ax[0].twinx(), *ax[1:]]

zEdges = [zMm[0] - 0.5 * (zMm[1] - zMm[0])]
for i in range(zMm.shape[0] - 1):
    zEdges.append(0.5 * (zMm[i] + zMm[i+1]))
zEdges.append(zMm[-1] + 0.5 * (zMm[-1] - zMm[-2]))

xPred = reverse['xPred']
xTrue = reverse['xTrue']

neEdges = np.linspace(8, 15, 101)
neIdxs = find_range_in_schema(schemas['inSchema'], 'ne')
tempEdges = np.linspace(3, 8, 101)
tempIdxs = find_range_in_schema(schemas['inSchema'], 'temperature')
velIdxs = find_range_in_schema(schemas['inSchema'], 'vel')
minVel = np.min(np.median(logvel_to_vel(xPred[:, velIdxs]), axis=0))
minVel = np.sign(minVel) * 2 * np.abs(minVel) if minVel <= 0 else 0.9 * minVel
maxVel = 2 * np.max(np.median(logvel_to_vel(xPred[:, velIdxs]), axis=0))
velEdges = np.linspace(minVel, maxVel, num=101)

ax[0].hist2d(np.concatenate([zMm] * xPred.shape[0]),
             xPred[:, neIdxs].reshape(-1),
             bins=(zEdges, neEdges), cmap=neColors, norm=PowerNorm(powerNormIdx))
ax[1].hist2d(np.concatenate([zMm] * xPred.shape[0]),
             xPred[:, tempIdxs].reshape(-1),
             bins=(zEdges, tempEdges), cmap=tempColors, norm=PowerNorm(powerNormIdx))
ax[0].plot(zMm, xTrue[0, 0], 'k--', linewidth=0.5)
ax[1].plot(zMm, xTrue[0, 1], 'k--', linewidth=0.5)
ax[0].set_xlim(None, 10.5)

ax[2].hist2d(np.concatenate([zMm] * xPred.shape[0]),
             logvel_to_vel(xPred[:, velIdxs].reshape(-1)),
             bins=(zEdges, velEdges), cmap=velColors, norm=PowerNorm(powerNormIdx))
ax[2].plot(zMm, logvel_to_vel(xTrue[0, 2]), 'k--', linewidth=0.5)
ax[2].set_xlim(None, 10.5)


ax[0].set_ylabel(r'$\log{n_e}$ [\si{\cm\tothe{-3}}]', c=cmapNe[-1])
ax[0].set_xlabel(r'$z$ [\si{\mega\metre}]')
ax[1].set_ylabel(r'$\log{T}$ [\si{\K}]', c=cmapTemp[-1])
ax[2].set_ylabel(r'$v$ [\si{\kilo\metre\per\second}]', c=cmapVel[-1])
ax[2].set_xlabel(r'$z$ [\si{\mega\metre}]')

fig.tight_layout()
latexFig = chRad.save_figure('RadynversionValidationReverse', fig, fext='.pgf')
latexFig.caption = 'Validation of the Radynversion Reverse Process'
\end{pycode}

\py[Radynversion]|chRad.get_figure('RadynversionValidationReverse')|


\subsection{Proof of Concept Results}