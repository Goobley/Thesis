\chapter{RADYNVERSION}\label{Chap:Radynversion}
%TC:group pycode 0 0
\begin{pycode}[Radynversion]
name = 'Radynversion'
chRad = texfigure.Manager(
    pytex,
    './04Radynversion',
    number=4,
    python_dir='./04Radynversion/python',
    fig_dir=   './04Radynversion/Figs',
    data_dir=  './Data/04Radynversion'
)
\end{pycode}

% \begin{itemize}
%     \item Introduction to inverse problems
%     \item Spectral line inversions
%     \item Introduction to machine learning
%     \item Model description
%     \item Model validation
%     \item 2014-09-06 flar results
% \end{itemize}

\emph{The contents of this chapter are based on my contributions to the research presented in \citet{Osborne2019}.}



\section{Introduction to Inverse Problems}

From the previous chapters we have an understanding of the so-called ``forwards-problem'' of RT.
The inverse of this problem is not well-posed; there is no guarantee of uniqueness, and typically the problem is extremely underdetermined.
The radiation field inside the plasma couples the atomic populations at all depths (as is evident from the form of the $\Lambda$ operator) in a way that cannot be trivially disentangled and information is then lost in the forwards process that is needed for the inverse process.

It is, of course, of great value to constrain the atmospheric parameters associated with a particular observation, and it is this we seek to achieve, by solving the inverse problem of radiative transfer.
As observed radiation is the only vector by which information can arrive from the Sun, it is important to maximally exploit the information that can be gleaned from observations and determine the structure of the atmosphere that produced the observed radiation.

\begin{figure}
\centering
\begin{tikzpicture}[line width=1pt, >=latex]
    \node (x1) {$T_1,\rho_1,\vec{v}_1, \vec{B}_1$};
    \node[below=of x1] (x2) {$T_2,\rho_2,\vec{v}_2, \vec{B}_2$};
    \node[below=of x2] (x3) {$T_3,\rho_3,\vec{v}_3, \vec{B}_3$};
    \node[below=of x3] (x4) {$T_4,\rho_4,\vec{v}_4, \vec{B}_4$};

    \node[above right=0.5cm and 4cm of x1] (y1) {$\vec{I}_1$};
    \node[below=of y1] (y2) {$\vec{I}_2$};
    \node[below=of y2] (y3) {$\vec{I}_3$};
    \node[below=of y3] (y4) {$\vec{I}_4$};
    \node[below=of y4] (y5) {$\vec{I}_5$};
    % \node[right=4cm of x4] (aux2) {};

    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(x1) (x4)}] (xSet) {};
    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(y1) (y5)}] (ySet) {};

    \node[below=2cm of y5,font=\bfseries, align=center] (yLabel) {$\mathcal{Y}$\\Observed Intensity};
    % \node[left=3cm of ylabel.center,font=\bfseries, align=center] {$\mathcal{X}$\\Atmospheric Parameters};
    \node[font=\bfseries, align=center] (xLabel) at (xSet |- yLabel){$\mathcal{X}$\\Atmospheric Parameters};

    % \draw[->] (xSet.90) to [out=50, in=150] (ySet.90) node[midway] {aa $f(\vec{x})$};
    % \draw[->, dotted] (ySet.270) to [out=210, in=310] (xSet.270) node[midway] {a $f^{-1}(\vec{x})$};
    \draw[->, out=50, in=150] (xSet.90) to node[below] {$f(x)$} (ySet.90);
    \draw[->, dotted, out=210, in=310] (ySet.270) to node[above] {$f^{-1}(y)$} (xSet.270);

    \draw[->] (x1) -- (y2.170);
    \draw[->] (x2) -- (y4.190);
    \draw[->, dotted] (y2.150) -- (x1.5) node[midway, above] {?};
    \draw[->, dotted] (y2.200) -- (x3.20) node[midway, below] {?};
    \draw[->] (x3) -- (y2.175);
    \draw[->] (x4) -- (y5.190);
\end{tikzpicture}
\caption{Degenerate Mapping}
\label{Fig:DegenerateMapping}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}[line width=1pt, >=latex]
    \node (x1) {$T_1,\rho_1,\vec{v}_1, \vec{B}_1$};
    \node[below=of x1] (x2) {$T_2,\rho_2,\vec{v}_2, \vec{B}_2$};
    \node[below=of x2] (x3) {$T_3,\rho_3,\vec{v}_3, \vec{B}_3$};
    \node[below=of x3] (x4) {$T_4,\rho_4,\vec{v}_4, \vec{B}_4$};
    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(x1) (x4)}] (xSet) {};

    \node[above right=1cm and 3.5cm of xSet.center] (y1) {$(\vec{I}_2, z_1)$};
    \node[below=of y1] (y2) {$(\vec{I}_4, z_1)$};
    \node[below=of y2] (y3) {$(\vec{I}_5, z_1)$};
    \node[right=0.5cm of y1] (y4) {$(\vec{I}_2, z_2)$};
    \node[below=of y4] (y5) {$(\vec{I}_4, z_2)$};
    \node[below=of y5] (y6) {$(\vec{I}_5, z_2)$};
    \node[right=0.5cm of y4] (y7) {$(\vec{I}_2, z_3)$};
    \node[below=of y7] (y8) {$(\vec{I}_4, z_3)$};
    \node[below=of y8] (y9) {$(\vec{I}_5, z_3)$};
    % \node[right=4cm of x4] (aux2) {};

    \node[shape=ellipse,draw=black,minimum size=3cm,fit={(y1) (y5) (y9)}] (ySet) {};

    \node[shape=circle, draw=black, right=2cm of y8] (prod) {$\times$};
    \draw[->, ultra thick] (prod.180) -- (ySet.0);

    \node[above=2cm of prod] (yy3) {$\vec{I}_5$};
    \node[above=0.5cm of yy3] (yy2) {$\vec{I}_4$};
    \node[above=0.5cm of yy2] (yy1) {$\vec{I}_2$};
    \node[shape=ellipse,draw=black,minimum size=1cm,fit={(yy1) (yy3)}] (yySet) {};
    \draw[->, ultra thick] (yySet.270) -- (prod.90);

    \node[below=2cm of prod] (z1) {$z_1$};
    \node[below=0.5cm of z1] (z2) {$z_2$};
    \node[below=0.5cm of z2] (z3) {$z_3$};
    \node[shape=ellipse,draw=black,minimum size=1cm,fit={(z1) (z3)}] (zSet) {};
    \draw[->, ultra thick] (zSet.90) -- (prod.270);

    \node[below=6cm of ySet.center,font=\bfseries, align=center] (yLabel) {$\mathcal{Y}\times\mathcal{Z}$\\Observed Intensity $\times$ Latent Space};
    \node[font=\bfseries, align=center] (xLabel) at (xSet |- yLabel){$\mathcal{X}$\\Atmospheric Parameters};
    \node[font=\bfseries, align=center] (zLabel) at (zSet |- yLabel){$\mathcal{Z}$\\Latent Space};
    \node[font=\bfseries, align=center, above=0.2cm of yySet.north] (yyLabel) {$\mathcal{Y}$\\Observed Intensity};

    % \node[below=2cm of y5,font=\bfseries, align=center] (ylabel) {$\mathcal{Y}$\\Observed Intensity};
    % \node[left=3cm of ylabel.center,font=\bfseries, align=center] {$\mathcal{X}$\\Atmospheric Parameters};

    % \draw[->] (xSet.90) to [out=50, in=150] (ySet.90) node[midway] {aa $f(\vec{x})$};
    % \draw[->, dotted] (ySet.270) to [out=210, in=310] (xSet.270) node[midway] {a $f^{-1}(\vec{x})$};
    \draw[->, out=50, in=100] (xSet.90) to node[below] {$g^{-1}(x)$} (ySet.90);
    \draw[->, dotted, out=260, in=310] (ySet.270) to node[above] {$g(y, z)$} (xSet.270);

    \draw[<->] (x1) -- (y4);
    \draw[<->] (x2) -- (y2);
    % \draw[->, dotted] (y2.150) -- (x1.5) node[midway, above] {?};
    % \draw[->, dotted] (y2.200) -- (x3.20) node[midway, below] {?};
    \draw[<->] (x3) -- (y7);
    \draw[<->] (x4.350) -- (y6.190);


\end{tikzpicture}
\caption{Bijective Mapping. For the sake of legibility we have only included the observed intensity vectors which were mapped to in Fig.~\ref{Fig:DegenerateMapping}.}
\label{Fig:BijectiveMapping}
\end{figure}

The forward process can be described by the diagram in Fig.~\ref{Fig:DegenerateMapping}.
This shows the theoretically degenerate nature of the problem; whilst the mapping from $\mathcal{X}$ to $\mathcal{Y}$ is always well-posed, the inverse is not, and with the information present in $\mathcal{Y}$ there is no way to immediately resolve this ambiguity (indicated by the question marks in this figure).
We can instead frame the problem as shown in Fig.~\ref{Fig:BijectiveMapping}, where elements of $\mathcal{Z}$ represent the information lost in the forwards process and a bijective mapping $g$ may be written between these spaces.
Clearly, $\mathcal{Z}$ is difficult to characterise, and we will discuss multiple approaches for replacing or reconstructing this information.
In the following, we denote individual samples of the atmospheric parameters $x$, the emergent line profiles $y$ and the latent space $z$.

\subsection{Milne-Eddington Inversions}

The loss of information can be somewhat limited by placing constraints on the solution. One of the simplest constraints that can be placed on the problem is that of a source function that changes linearly with continuum optical depth, whilst all other parameters are held constant throughout the atmosphere.
This is a low-order approximation of the problem, and is convenient as we can express compute the outgoing intensity analytically.
This can be done for the full Stokes polarised case of the RTE, but for illustration we choose to use only the scalar case here.
With continuum opacity $\tau_c$ the source function is then defined as
\begin{equation}
    S(\tau_c) = S_0 + S_1 \tau_c,
\end{equation}
where $S_0$ and $S_1$ are constants.
In this situation where we are dealing with the source function directly, rather than atomic parameters (as we are effectively in an two-level atom case), we can define the line strength $\alpha$ as the ratio between the continuum opacity and the line opacity i.e.
\begin{equation}
    \tau(\nu) = \tau_c (1 + \alpha \phi(\nu)),
\end{equation}
where $\phi$ is the line profile.

Now, by integrating the RTE directly and assuming as semi-infinite atmosphere with no light entering the boundary at $\infty$ we obtain the outgoing intensity
\begin{equation}
I(\tau=0, \nu) = S_0 + \frac{S_1}{1 + \alpha\phi(\nu)}.
\end{equation}

Thanks to the analytic nature of this solution, it is easy to attempt to fit this to observations, and effects such as constant atmospheric velocity, and magnetic field can be included in the line profile.
This approach is rapid and works quite well for quiet photospheric lines, such as Fe I 6301 \AA{} and Fe I 6173 \AA{} as used in Hinode SOT and HMI \citep[e.g.][]{Centeno2014}.

The linear description of source function reduces its applicability to chromospheric lines.
However, variants of this approach have implemented more complex shapes for the source function as a function of optical depth, and have had greater success approximating chromospheric lines than the pure linear approximation \NeedRef{} del Toro Iniesta?.

\subsection{Generalisation}

There are substantial limitations to imposing a chosen source function.
A far more flexible approach is to attempt to fit a source function.
From the previous discussion of formal solvers we know that for a discretised atmosphere wherein the source function follows a prescribed functional form over each interval we can write
\begin{equation}
    I(\tau=0) = \sum_i \int_{\tau_i}^{\tau_{i+1}} S(\tau)e^{-\tau}\, d\tau,
\end{equation}
whereby we have once again assumed that $I(\tau=\infty)=0$.
If the source function is taken to be constant in each slab this becomes
\begin{equation}\label{Eq:LinearResponse}
    I(\tau=0) = \sum_i S(\tau_i) \int_{\tau_i}^{\tau_{i+1}} e^{-\tau}\, d\tau,
\end{equation}
and for each slab, the associated integral represents its contribution to the outgoing intensity. In fact, as the each contribution is linear in the source function, it represents the \emph{response function} here i.e. the response in the outgoing intensity to a perturbation in the source function at this depth.
We can therefore form a response matrix associating, for each wavelength and depth, the response to a change in source function at this depth.
This response function does not change with the source function due to the linearity of the problem.
In theory we should therefore be able to infer the depth stratified form of the source function, however two primary difficulties arise.

Typically observations of a line contain many more wavelength points than the number of depth points it is feasible to have in our discretised source function, leading to an overdetermined system with no direct inverse.
Even if one were to modify the system so that these dimensions are compatible, the response matrix typically has extremely poor conditioning and cannot be inverted directly.
An immediate solution is then to use a pseudoinverse based on the singular value decomposition of the response matrix, as this can attempt to tackle both problems at once.
% Nevertheless, this tends to produce oscillatory solutions, as shown in Fig. \NeedRef{}.
We shall demonstrate this with a numerical example, first defining a function giving the response at each depth as per \eqref{Eq:LinearResponse}:

\begin{pyblock}[Radynversion]
def linear_response_fn(tau, alpha, phi):
    dtau = np.gradient(tau)
    lineDepthIncrease = 1.0 + alpha * phi[None, :]

    responseFn = np.exp(-tau[:, None] * lineDepthIncrease) \
                  * dtau[:, None] * lineDepthIncrease
    return responseFn
\end{pyblock}

Here, we are using an atmosphere defined in terms of optical depth \texttt{tau}, constant line to continuum opacity ratio \texttt{alpha}, and line profile \texttt{phi}.
This function can then be applied to a source function quadratically varying in $\log \tau$:

\begin{pyblock}[Radynversion]
import lightweaver as lw
Ndepth = 101 # Number of depth points in the stratification
tau = 10**np.linspace(-5, 1, Ndepth) # Optical depth stratification
alpha = 30 # Line strength relative to continuum
x = np.linspace(-20, 20, 101) # Line spectral grid in Doppler units
phi = lw.voigt_H(1.0, x) / np.sqrt(np.pi) # Voigt line profile

# Quadratic Source function peaking at logTau = -1,
# strictly positive and normalised.
S = -(np.log10(tau) + 1)**2
S -= 1.2*S.min()
S /= S.max()

# Compute response function
responseFn = linear_response_fn(tau, alpha, phi)
# Compute outgoing intensity as per equation
Iout = np.sum(S[:, None] * responseFn, axis=0)
# Inferred source function from outgoing intensity and
# response function (via pseudoinverse)
Sinferred = np.linalg.pinv(responseFn.T) @ Iout
\end{pyblock}

\begin{pycode}[Radynversion]
fig = plt.figure()
ax = plt.gca()
plt.plot(np.log10(tau), S, label='True source function')
plt.plot(np.log10(tau), Sinferred, label='Psuedoinverse inferred source function')
plt.xlabel(r'$\log\tau_c$')
plt.ylabel(r'S')
plt.legend(frameon=False, loc='lower right')

lFig = chRad.save_figure('InferredSourceFn', fig, fext='.pgf')
lFig.caption = r'Comparison between true source function and inferred for the case of a quadratic variation.'
\end{pycode}

\py[Radynversion]|chRad.get_figure('InferredSourceFn')|

The true and inferred source functions are shown in Fig.~\ref{Fig:InferredSourceFn}.
The poor quality of the pseudoinverse solution is apparent due to the large oscillations and poor agreement with the true source function, and this example showcases the simple case of a static atmosphere with constant line strength.
If these parameters are also allowed to vary then the solution is likely to deteriorate further.

% Therefore, there is a need for \emph{regularisation} of the solution, penalising deviations from smooth solutions.
% This can be achieved in several ways, a common choice of which is Tikhonov regularisation.
Whilst this problem can be overcome (to some extent) by regularisation of the solution, to enforce smoothness in the ill-posed problem, we are also left with a problem of interpretation for NLTE problems.
Ultimately, we seek to learn information about the atmospheric structure, and not simply the source function.
In LTE, where the source function is set by the local atmospheric parameters, this approach has long been viable.
The SIR inversion code \citep{1992RuizCobo} analytically computes the (full Stokes) response functions (based on the formulations by \citet{SanchezAlmeida1992}) to perturbations in different atmospheric parameters at the same time as the formal solution.
These response functions are used in conjunction with a Levenberg-Marquadt damped least squares regression procedure to modify the starting atmosphere (defined on equidistant nodes in $\log \tau$ assumed to follow cubic splines between nodes) until the synthesised radiation matches the observation as closely as possible.

For lines that are formed well outside LTE the process of determining the response functions to atmospheric perturbations is significantly more arduous.
The most common approach has been to apply a finite difference method to the outgoing radiation from the statistical equilibrium solution to an atmosphere by successively perturbing each parameter at each node in the atmosphere.
This is incredibly resource intensive, but has reliably been used since the NICOLE code \citep[developed from \citet{SocasNavarro2000} but no longer using fixed departure coefficients]{Socas-Navarro2015}.
The Stockholm Inversion Code (STiC) also follows this procedure, using a modified form of RH, allowing for the application of PRD \citep{2019dlcr}.

Analytic response functions for the multi-level NLTE problem were first derived by \citet{Milic2017}, and are now implemented in the SNAPI code \citep{Milic2018}.
These response functions should significantly reduce the computational cost of NLTE inversions, a necessity for current and next generation solar observations.

An alternative approach to reducing the computational overhead of NLTE inversions can be seen in the DeSIRe code, which combines SIR and RH, guiding itself to an approximate solution using the fast analytic LTE response functions of SIR and fine-tuning the solution with finite-difference response functions computed with RH. \NeedRef{} {\color{Red} This paper seems to be in limbo?}

All of the codes discussed here use the Levenberg-Marquadt regression method with different varieties of regularisation to enforce smooth solutions.
Additionally, only the statistical equilibrium solution is considered, and then only in hydrostatic equilibrium as this reduces the number of parameters to be inferred.
It is common to allow line-of-sight velocity as a parameter, which technically violates the constraint of hydrostatic equilibrium, however this is a minor effect and is seed as a worthwhile trade-off for the increase in tractability of quiet sun inversions.
Clearly these constraints render this technique very difficult to apply to flares, although NICOLE has been applied to flaring atmospheres by \citet{Kuridze2018}.

An technique related to inversions that has commonly been applied to flares in the last decade is that of forward modelling through the use of RHD codes.
Where possible the energy input is constrained from observations, via techniques such as X-ray spectroscopy to deduce the non-thermal electron flux and spectral index.
A challenging manual iteration technique then follows to attempt to obtain an agreement between the time-dependent simulation and the observations.
This is extremely time-consuming both due to the manual aspect of the inversions and the computational requirements of the RHD simulations.
Clearly, the human intervention necessary to optimise and analyse these simulations cannot scale to the large volumes of data coming already present and coming from future telescopes.
This technique has nevertheless yielded many interesting developments in our understanding of the structure of the flaring chromosphere from the investigation of spectral line shapes and continuum enhancements \citep{Kuridze2015,RubioDaCosta2016, Kowalski2017,Simoes2017}.

Returning now to the mathematical description of an inversion we can start to discuss the meaning of $z$ in practice.
With the response function based inversions described previously, the size of $\mathcal{Z}$ is limited by the constraints placed on the atmospheric stratification, and the regularisation thereof.
It then becomes feasible to ``explore'' this space using the gradient information from the response functions to guide the solution.
It is worth noting that this approach does not guarantee the global minimum solution; whilst Levenberg-Marquadt is extremely efficient at finding local minima, it provides no further guarantees and the final solution may therefore be substantially influenced by the choice of starting atmosphere, which is typically made in an ad hoc fashion.
The RHD based methods are also comparable in terms of exploration of $z$, except here the optimisation is done manually and the gradient information is replaced by intuition.

As discussed previously, the assumptions that render NLTE response function based approach tractable, such as hydrostatic equilibrium, cannot be applied in flares where flows often approach the sound speed.
We therefore need an inversion technique that can operate outside of these constraints.
Our standard RT forwards process can be framed as a function $y = f(x)$ with atmospheric inputs $x$ and line profiles $y$.
Clearly this function is not bijective, but if we also capture the information lost in the forward process, we can instead define a bijective function $x = g(y, z)$ such that $g^{-1}$ represents the forwards process, and $g$ the inverse process.
Our theory of radiative transfer does not give any immediate insight into the formulation of $g$ with so few constraints, so we instead turn to the field of machine learning.

\section{Introduction to Machine Learning}

Machine learning describes a family of generic algorithms that are used to make sense of data without being explicitly programmed.
A model is defined by the researcher, but its final behaviour is determined by patterns in the data it is fed.
The abundance of both observational and simulated solar data continues to increase and new approaches, such as machine learning, are needed to make use of this vast quantity of information in a computationally tractable manner, helping to highlight patterns that can be further investigated by researchers.

There are three primary varieties of machine learning algorithms: supervised, semi-supervised, and unsupervised learning.
Supervised algorithms are the most common.
The model is provided with a set of of examples (typically produced or preprocessed manually) and is then trained so that it represents an approximate transformation between the input and output data defined by the training data.
We can further divide this class into classification and regression models.
Classification associates each class with a discrete input, possibly labelling an image based on its contents, whereas regression approximates a continuous mathematical function.
In both of these cases the model approximates a function which is learnt entirely from the training data.

Unsupervised learning does not require the manually prepared set of examples, but instead organises data based on generic programmed criteria.
Two commonly used examples of unsupervised learning are clustering and dimensionality reduction techniques.
Clustering algorithms extract groups of similar objects (where similar is defined given a particular basis and metric determined by the choice of algorithm), and can be used to find patterns in large datasets.
There are many kinds of dimensionality reduction techniques, but one of the most common and general choices is principal component analysis, where an orthogonal basis spanning the data is constructed and then sorted by the variance of the factors of each of these axes (i.e. the eigenvalues of the covariance matrix).
For data of dimensionality $m$, keeping $m$ principal components allows for a perfect reconstruction, as this is simply a basis transformation, however, we can often discard terms with small variance and produce accurate approximate reconstructions of the data with substantially fewer than $m$ components.
It is necessary to ensure that sufficient components are chosen for the reconstruction to be accurate, but such techniques can reveal patterns that are otherwise difficult to discern in the original high-dimensional spaces.

Finally, as implied by the name, semi-supervised learning lies in between the two previously discussed classes.
It still requires preprocessed training data which is used for some training, however unsupervised learning processes may be used internally to the model, or in some cases data generated by a model is used in conjunction with this training data.
This form of machine learning exists only within the realm of deep learning, built on neural networks.

\subsection{Artificial Neural Networks}

Artificial Neural Networks (ANNs) loosely follow the principle of biological neuronal systems, consisting of layers of interconnected neurons, the output of which is summed in synapses and then has a non-linear activation function applied to determine if the signal is passed on through the network.
ANNs consists of multiple layers of neurons and synapses whereby we designate any layer that is neither the input nor the output a \emph{hidden layer}.
If an ANN consists of more than one hidden layer it is termed a deep neural network (DNN), and these are considered to be the standard building blocks of modern machine learning \citep{Raschka2015}.

There are many different architectures for ANNs, used for solving different problems.
ANNs can vary in number of hidden layers, interconnectedness of neurons within these layers, connectedness of the layers to each other, and the activation function used in each layer.
We distinguish two primary forms of layers, based on their interconnectivity; these are fully connected (FC) where each neuron in a layer is the linear combination of its inputs (typically the activation function applied to the neurons of the previous layer), and the convolutional layers of convolutional neural networks (CNNs, \citet{1998Lecun,2003Simard}) which connect only nearby neurons to exploit local structure in the input (in one or more dimensions).
These convolutional layers can then be described as a set of filters learned during the training process which are cross-correlated with the input, the output of which is then passed through the activation function.
CNNs are somewhat inspired by the neuronal structure of the visual cortex, and have been applied with great success in the fields of image analysis, processing, and generation \citep{Raschka2015}.
A fully connected layer rarely works well for these tasks as a slight movement of an object within an image can easily invalidate its training whereas the smaller layers of a CNN sweep across the image (are applied to each region in turn) and are far less affected by this.

There are many common forms of activation function.
Due to the backpropagation method used for training ANNs it is highly advantageous if these non-linear activation functions be trivially differentiable.
Some common choices are the sigmoid function
\begin{equation}
    \mathrm{S}(x) = \frac{1}{1+e^{-x}},
\end{equation}
inverse tangent $\tan^{-1}(x)$, and variants of the rectified linear unit (ReLU; \citet{2010Nair}).
\begin{equation}
    \mathrm{ReLU}(x) = \mathrm{max}(0, x).
\end{equation}
All of these functions are used in the creation of ANN based models, but the ReLU family is key to modern machine learning for reasons of sparsity in its output.
Here sparsity refers to the presence of zeros in the output of a layer creating clearer pathways through the network.
Classification ANNs will typically employ an activation function on the output layer (most frequently a normalised exponential to select a single discrete class), whereas ANNs employed in regression problems will rarely do so.

\subsection{General Function Approximations}

ANNs are universal function approximators; they can learn arbitrarily complex classification and regression problems \citep{Rumelhart1986,1989Cybenko}.
This was theoretically proven for shallow neural networks (with only one hidden layer) using sigmoidal activation functions by \citet{1989Cybenko}.
Increasing the precision to which a function is approximated may however require exponential increases in layer width and training.
A similar proof for the commonly used ReLU activation function was provided by \citet{Lu2017}, who investigated the bounded expressions for the layer width and network depth needed to approximate functions to arbitrary precision.
Unfortunately these results can be difficult to apply to many real world scenarios where the intrinsic dimensionality of the function being approximated is not known (these results are also affected by any imperfections in the training data).

It is also possible to increase the approximation capability of an ANN by increasing its depth (the number of stacked layers), these stacked layers then represent the composition of functions, and each additional layer increases the complexity of the representation of its input, allowing for very complex tasks to be approximated \citep{Raschka2015}.
The approximation power of stacked layers explains why the DNN is core to modern machine learning, however care must be taken when designing a model to select appropriate width and depth for the problem at hand \citep{Lu2017}.

\subsection{Training via backpropagation}

ANNs are trained via a process known as backpropagation \citep{Rumelhart1986}.
The networks are composed of linear combinations and (by our original requirements) differentiable activation functions.
The entire network can then be differentiated by repeated applications of the chain rule (from output to input) to find the gradients of the output with respect to each weight (the coefficients of the linear combinations in each layer) and input, which then describes how each weight affects the output.
Typically the output of the network when fed with data from the training set is compared against the expected output via a loss function, and then the gradient information from this loss is combined with the previous gradient of the network output to each weight and used to minimise the magnitude of the loss by modifying the weights.

Updating the weights in the network can be carried out in a variety of ways, but it is a similar minimisation process to that used in inversions.
The basic method is that of stochastic gradient descent (SGD) which takes a step through the loss space guided by the gradients for each batch of training data.
The size of this step is known as the learning rate, and is a \emph{hyperparameter}\footnote{Hyperparameters are tunable parameters that are often set by the researcher, or optimised by a process external to the training of the INN} of the ANN.
It can be kept constant, vary following a prescribed evolution with epoch, or even be modified based on the rate of convergence of the training procedure.
As SGD is only affected by the most recent batch of data it can have difficulty escaping local minima and traversing plateaus in the loss space.

Many improvements to SGD have been developed, such as the addition of momentum, which accelerates convergence and helps to avoid the solution being overly affected by a single batch of training data.
Other modern algorithms based on the same principles as SGD have also been developed \citep[e.g. the Adam algorithm,][]{2014Kingma} and often converge in fewer epochs (rounds of training) to similar or better solutions.
None of these stochastic algorithms can guarantee a global minimum in the loss space, and such a requirement is not feasible for anything other than the smallest neural networks, where more time- and memory-consuming optimisers can be used due to the much more dimensionally compact spaces over which the optimsation occurs.
Nevertheless, with sufficient training data and epochs a model capable of approximating the function we wish to learn should be able to descend into sufficiently good local minimum using these techniques.

Auxiliary techniques to improve model convergence have also been developed, such as minibatching, in which the network is only shown a random portion of the training data each epoch.
Clearly this can reduce the computational cost of an epoch, as fewer calculations are performed on this training set, but minibatching can also improve the convergence by avoiding the stagnation that arises in the traditional batched gradient descent where the entire training set is used to direct the step.

A technique known as autodifferentiation has become prominent in the field of machine learning.
It allows users to easily design custom blocks and compose these without the need to consider the implementation of the derivatives needed for training as these are computed by the framework.
Frameworks (e.g. TensorFlow, PyTorch \NeedRef{}) may record the path of data through a network and then using this information (as every function present therein is differentiable) construct all necessary gradient information for training, which can then be computed on GPU.
This automatic nature of this approach has enabled the rate of development seen in machine learning in the last decade, as it allows researchers to spend longer thinking about design than low-level engineering.

\subsection{Difficulties training DNNs}

As the number of layers in an ANN increases the networks can become much harder to train; the gradient of the output with respect to the weights in early layers can easily become vanishingly small due to the repeated multiplication of small gradients in the deeper layers.
The use of ReLU activation functions often minimises this effect, but can instead lead to exploding gradients due to their high dynamic range.
\citet{2015He} developed residual networks (ResNets), which have greatly increased the depth and complexity of networks that can be effectively trained, and now networks with many hundreds of layers are frequently used \citep[e.g.][]{Jegou2017}.
The residual blocks of these networks contain so-called skip connections, which take the output from a layer and sum or concatenate it with the output of layer one or more levels deeper.
These skip connections provide a path for gradients to propagate through the network, helping to avoid both vanishing and exploding gradients.
Variants of the ReLU function are almost uniquely used in ResNets as these additionally provide sparsity to the representation (i.e. their output is 0 for all input less than or equal to 0), which can improve the expressiveness\footnote{The expressive power of a neural network is its ability to approximate functions.} of the representation and aid in disentangling information propagating through the network \citep{Glorot2011}.
These variants include the leaky ReLU \citep[$\max(0.01x, x)$;][]{Maas2013} which still produces a small amount of gradient information for negative inputs, helping to prevent neurons with ReLU activation from ``dying'', and exponential linear units \citep[ELUs;][]{Clevert2015} which achieve a similar result in a smoothly varying fashion.

Like all regression models with a large number of free parameters, ANNs can very easily enter a regime of overfitting their training set.
In this situation the ANN has learnt to match its training set so closely that it is unlikely to perform reliably on inference of unseen data.
This can often manifest as memorisation, where the network has learnt to produce the expected output for a training sample, but not the relationship between the two.
ANNs must therefore be trained with care and diligent use of validation data, prepared in the same way as the training set, but never shown to the network during training.
The network's performance can be judged by how well it performs in inference on the validation set in between training epochs.
If the performance on the training data continues to improve over time, but the performance on the validation set stagnates or worsens then the network has entered an overfitting regime.

There are additional techniques that can be employed to mitigate overfitting, such as regularisation, which will attempt to prevent a model's weights from minimising the loss function too perfectly, for example by penalising overly large weights with a modified loss function, or randomly deactivating neurons in each layer during training (this approach is known as \emph{dropout}).

Selecting hyperparameters for a model can be a challenging process of manual optimisation but is essential to training, and many advanced optimisers like Adam require additional hyperparameters that can drastically influence the rate of convergence.
Approaches such as grid searches can be applied here, but given the computational requirements of training these models, an intuitive approach is often applied.

\section{The RADYNVERSION Model}\label{Sec:RadynversionModel}

Our approach towards inversions of solar flares is based upon the application of machine learning to the problem as it is framed in Fig.~\ref{Fig:BijectiveMapping}.
We wish to learn the form of the latent space, and by sampling this space sufficiently for a given observation we can infer the likelihoods of parameters of the flaring atmosphere.
Nevertheless, we have no data on which to train which directly characterises the latent space, amd thus we turn to the technique of invertible neural networks (INNs) which naturally learn bijective functions to learn the bijective mapping \emph{and} the form of the latent space simultaneously.

Our INN is trained using RHD simulations generated with \Radyn{}, which contain the structure of a model flaring atmosphere and the synthesised emergent radiation (here the \Ha{} and \CaLine{} spectral lines).
As the function $x = g(y, z)$ (as shown in Fig.~\ref{Fig:BijectiveMapping}) is bijective, our INN can learn the function $g^{-1}$ as the forwards process, and $g$ as the inverse process.
ANNs can learn to approximate any function, so they can be trained to transform any distribution to any other.
We can therefore choose to characterise the latent space as any smooth continuous distribution desired and the INN will internally learn the mapping to the form of the ``true'' latent space contained within the simulations.
The approach to inferring atmospheric parameters using the INN therefore differs to the approach of traditional methods that take guided explorations of the latent space.
Due to the speed of the INN solution relative to the cost of formal solution and iteration that is required in conventional NLTE inversions, we can simply take a large number of samplings of our chosen latent space, and thus generate a probability density function for the atmospheric parameters at each location in the atmosphere, conditioned by our training set.
For simplicity we choose to represent the latent space as the unit multivariate Gaussian distribution with mean 0 and variance 1, which we denote $\mathcal{N}(0, \mathcal{I}_n)$ for an $n$ dimensional case.

The INN is a form of DNN in which invertible blocks are used.
A traditional fully connected or convolutional layer is not generally invertible.
Indeed, whilst a fully connected layer with equal number of inputs and outputs, and an invertible activation function \emph{can} be inverted, it is extremely computationally expensive to do so due to the cost of inverting the potentially large square matrix of weights.
Instead, we use \emph{affine coupling layers} \citep{2014Dinh,2016Dinh} which are trivially reversible, and thus during the training of the forward process the inverse of this learned function is simultaneously trained ``for free''.
The affine coupling layers used in our INN were first presented by \citet{2018Ardizzone}.
The input vector $\vec{x}$ is first split into two halves $[x_1, x_2]$ which undergo the following affine transformations
\begin{align}
    y_1 &= x_1 \otimes \exp(s_2(x_2)) + t_2(x_2),\\
    y_2 &= x_2 \otimes \exp(s_1(y_1)) + t_1(y_1),
\end{align}
where $\otimes$ represents the elementwise product of tensors and $s_i$ and $t_i$ ($i \in \{1, 2\}$) are arbitrarily complex differentiable functions (that need not be invertible).
It is worth noting the order of operations here, as $y_1$ must be computed before $y_2$.
The output $\vec{y}$ is then constructed from the concatenation of $y_1$ and $y_2$.
The inverse of these affine transforms is given by
\begin{align}
    x_2 &= (y_2 - t_1(y_1)) \oslash \exp(s_1(y_1)),\\
    x_1 &= (y_1 - t_2(x_2)) \oslash \exp(s_2(x_2)),
\end{align}
where $\oslash$ represents the elementwise division of tensors.
We have stated that the $s_i$ and $t_i$ functions can be of arbitrary complexity, but clearly they need to be tailored to the particular task and for this reason we apply DNNs in this role.
The networks used for $s_i$ and $t_i$ are identical, save for the application of a $\tan^{-1}$ transformation on the output of the $s_i$ block, which prevents extreme values from being produced whereby the exponential term would dominate, or have no effect on the output of the block.

In the spirit of DNNs we then stack multiple affine coupling layers to allow for increased representational capability within the INN.
As the the input is split in half upon entering each affine layer, we can see that the data in each half is only combined at the elementwise multiplication step in each layer on its journey through the network.
To alleviate this, and further increase the generalisation capabilities of the network, we interleave a permutation layer between each affine coupling layer.
This layer shuffles the data in a random, but fixed order which is different for each permutation layer, before it is split, whilst allowing for trivial reversibility.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[line width=1pt, >=latex,
    block/.style={
    draw,
    rectangle,
    minimum width={width("Affine Coupling Layer")+2pt},
    font=\small}]
    \node [draw, anchor=center] (Input) {Input};

    \node [block, right=1cm of Input.0, rotate=90, anchor=north] (Acl1) {Affine Coupling Layer};
    \node [block, right=of Acl1.north, rotate=90, anchor=north] (P1) {Permutation Layer};

    \node [block, right=of P1.north, rotate=90, anchor=north] (Acl2) {Affine Coupling Layer};
    \node [block, right=of Acl2.north, rotate=90, anchor=north] (P2) {Permutation Layer};

    \node [block, right=of P2.north, rotate=90, anchor=north] (Acl3) {Affine Coupling Layer};
    \node [block, right=of Acl3.north, rotate=90, anchor=north] (P3) {Permutation Layer};

    \node [block, right=of P3.north, rotate=90, anchor=north] (Acl4) {Affine Coupling Layer};
    \node [block, right=of Acl4.north, rotate=90, anchor=north] (P4) {Permutation Layer};

    \node [block, right=of P4.north, rotate=90, anchor=north] (Acl5) {Affine Coupling Layer};

    \node [draw, below right=1cm and 1cm of Acl5.center, anchor=180] (LatentSpace) {Latent Space};
    \node [draw,
           minimum width={width("Latent Space")+2pt},
           above right=1cm and 1cm of Acl5.center, anchor=180] (Output) {Output};

    \draw[->, color=TolBlue] (Input.20) to (Input.20 -| Acl1.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl1.south) -- (Input.20 -| P1.north);
    \draw[->, color=TolBlue] (Input.20 -| P1.south)   -- (Input.20 -| Acl2.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl2.south) -- (Input.20 -| P2.north);
    \draw[->, color=TolBlue] (Input.20 -| P2.south)   -- (Input.20 -| Acl3.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl3.south) -- (Input.20 -| P3.north);
    \draw[->, color=TolBlue] (Input.20 -| P3.south)   -- (Input.20 -| Acl4.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl4.south) -- (Input.20 -| P4.north);
    \draw[->, color=TolBlue] (Input.20 -| P4.south)   -- (Input.20 -| Acl5.north);
    \draw[->, color=TolBlue] (Input.20 -| Acl5.south) to[out=0, in=180] (Output.180);

    \node [draw, circle, fill, inner sep=0pt, minimum size=2pt,
           right=2cm of Acl5.center] (Mix) {};

    \draw[->, color=TolTeal] (Output.south -| Mix) -- (Mix);
    \draw[->, color=TolTeal] (LatentSpace.north -| Mix) -- (Mix);
    \draw[->, color=TolTeal] (Mix) to[out=180, in=0] (Input.340 -| Acl5.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl5.north) -- (Input.340 -| P4.south);
    \draw[->, color=TolTeal] (Input.340 -| P4.north) -- (Input.340 -| Acl4.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl4.north) -- (Input.340 -| P3.south);
    \draw[->, color=TolTeal] (Input.340 -| P3.north) -- (Input.340 -| Acl3.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl3.north) -- (Input.340 -| P2.south);
    \draw[->, color=TolTeal] (Input.340 -| P2.north) -- (Input.340 -| Acl2.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl2.north) -- (Input.340 -| P1.south);
    \draw[->, color=TolTeal] (Input.340 -| P1.north) -- (Input.340 -| Acl1.south);
    \draw[->, color=TolTeal] (Input.340 -| Acl1.north) -- (Input.340);
\end{tikzpicture}
\caption{Structure of the final RADYNVERSION network. The forwards process is shown with the {\color{TolBlue}blue} arrows, and the inverse process with {\color{TolTeal}teal} arrows.}
\label{Fig:RadynversionDiagram}
\end{figure}

For the RADYNVERSION\footnote{RADYNVERSION is a portmanteau of \Radyn{} and ``inversion''.} model we use five affine coupling layers, with four interleaved permutation layers.
This is shown schematically in Fig. \ref{Fig:RadynversionDiagram}.
Given the presence of four DNNs per affine coupling layer, our final model is composed of twenty DNNs.
Each of these networks is an individual four-layer fully connected network utilising leaky ReLU activation functions after each of the first three layers, and a ReLU following the final layer.

The input to the RADYNVERSION network then consists of a reduced set of atmospheric parameters at a given point in time, in our case the temperature $T$, electron density $n_e$, and velocity $v$.
These are provided to the network on a fixed height stratification, with 50 points covering the entire atmosphere.
The large dynamic range of these parameters (both across a single atmosphere, and between different timesteps at a fixed location in the atmosphere) can have a negative impact on the training and accuracy of the ANN, and we therefore map $T\mapsto\log_{10} T$, $n_e\mapsto\log_{10} n_e$, and $v\mapsto\mathrm{sign}(v)\log_{10} \left( |v_{\mathrm{km\, s^{-1}}}| + 1\right)$.
The mapping for $v$ serves to scale it based on its base-10 logarithmic value, whilst preserving its sign information.
These logarithmic mappings preserve detail better over each decade than a simple linear rescaling.

The output consists of line profiles, in this case \Ha{} and \CaLine{}.
Both of these are interpolated onto fixed wavelength grids, with 30 points each (a half width of \SI{0.14}{\nano\metre} for \Ha{} and \SI{0.1}{\nano\metre} for \CaLine{}).
The intensity values of the spectral lines are scaled to cover the range [0, 1] whilst preserving the relative intensity of the two (which conveys important information regarding continuum emission).

Our model then has an input dimensionality of 150 and an output dimensionality of 60.
We choose, by experimentation, to set the size of the latent space to same as the input, however we cannot prove the opimality of such a choice as it depends on the (unknown) intrinsic dimensionality of the problem.
Thus the input and total output (i.e. output and latent space) must be of length at least 210.
To improve the generalisation performance of the network and allow it a greater dimensionality for its representation of the data we choose to set the input and output size to 384.
The input to the network is zero-padded to this length, and the output and latent space are concatenated with zero-padding in between.
The deep neural networks used inside the affine coupling layers have an input and output of length 192, but this is increased to 384 for the inner layers, to further increase their representational ability.

The RADYNVERSION model presented here considers each set of atmospheric parameters and observables as instantaneous quantities.
As we have discussed, it is usually necessary to consider time-dependent populations in flares, and clearly we do not do that here.
The atomic populations are not considered directly with this model, and we are interested in whether there is sufficient information present in this reduced description of the atmosphere to reproduce the emergent line profiles and whether an ANN can then learn to decode this information.

\subsection{Training Data}

To train this model we use 81 \Radyn{} simulations computed by the F-CHROMA project \NeedRef{} {\color{Red} Let's see what happens to the online database}.
These models all start from a variant of the VAL3C quiet sun atmosphere \citep{Vernazza1981}, slightly modified to remain stable in \Radyn{}.
All models are heated by a symmetric triangular electron beam pulse, modelled using the Fokker-Planck module (with an initial power law distribution of electron energies), of \SI{20}{\s} duration, with a peak at \SI{10}{\s}.
The total beam deposition varies between \SI{3e10}{\erg\per\square\cm} and \SI{1e12}{\erg\per\square\cm}, the low-energy cutoff is one of \{10, 15, 20, 25\}~\si{\kilo\electronvolt}, and the spectral index of the electron energy distribution is one of \{3, 4, 5, 6, 7, 8\}.
All of these simulations last for \SI{50}{\s}, with data saved every \SI{0.1}{\s}.

Not all combinations of these parameters converged, in particular some simulations with lower values for the low-energy cutoff, higher spectral indices, and high total energy deposition were not present in the grid.
From the 81 simulations we then have 40,500 individual timesteps of which we separate 20\% for validation purposes.
The atmospheric parameters and line profiles are mapped onto their fixed grids and prepared as discussed in Section~\ref{Sec:RadynversionModel}.
Our height stratification is chosen to primarily sample the chromosphere, and places 45 linearly spaced points below \SI{3.5}{\mega\metre}, with a constant spacing of \SI{79.2}{\kilo\metre}.
The remaining 5 points are then expontially spread through the corona from \SIrange{3.5}{10}{\mega\metre}.

\subsection{Training Method}

Our training method is based on the one presented in \citet{2018Ardizzone} and makes the network is constructed using their framework. \NeedRef{}
The INN is trained in both directions to ensure the conditioning of both the forwards and inverse problems.
The model is trained using minibatching with the same minibatch of the training set being used in both directions.
Both training directions are also constrained by two loss functions.
The forwards direction (from atmospheric parameters to line profiles) uses an L2 loss ($||y-y_\mathrm{true}||_2^2$, where $y$ indicates the output of the network and $y_\mathrm{true}$ the expected output) on the output vector of line profiles and zero padding.
This latent space is constrained by a Maximum Mean Discrepancy (MMD) loss.
The MMD is a loss that compares distributions from finite samples, and is computed between batches of [$y$, $z$] and [$y_\mathrm{true}$, $\mathcal{N}(0, \mathcal{I}_z)$].
This is discussed in depth, along with implementation details in Section~\ref{Sec:Mmd}.
During the forward process the MMD loss is used to ensure that the network learns to map the true latent space to our chosen form for it (the multivariate unit Gaussian distribution).
A traditional regression loss cannot be applied here, as we would have to assign fixed ``samples'' from the latent space for each timestep in the training set, which cannot be done without understanding the true latent space.
Its aim is instead to condition the distribution produced in the latent space.
To this end, whilst $y$ is included in the MMD loss terms (as this is an important component of the output), the gradients on $y$ due to the MMD loss are ignored, so as not to affect the training of the forwards model.
For both the L2 and MMD losses to converge on the forwards process ensures that samples of $z$ are correctly independent of $y$ as they must not contain copies of the same information for the reverse process to work correctly.

The reverse process is trained similarly, with an additional two losses.
An L2 loss is used for $x$ and the zero-padding to ensure the expected atmosphere parameters are produced (and that the padding remain 0), and an MMD loss ensuring the correct distribution of $x$ for random latent samples.

Both of the forwards and backwards losses are linearly combined to produce a set of weighted gradients used to update the network.
We define three hyperparameter weights for this purpose $w_\mathrm{pred}$, $w_\mathrm{latent}$, and $w_\mathrm{rev}$.
These are combined to produce the losses
\begin{align}
    loss_f &= w_\mathrm{pred} L2_f + w_\mathrm{latent} MMD_f,\\
    loss_b &= 0.5w_\mathrm{pred} L2_b + \xi(n)w_\mathrm{rev} MMD_b,
\end{align}
where $f$ and $b$ represent the forwards and backwards terms respectively, and $\xi$ is a term that gradually increases towards unity so as to limit the impact of the backwards MMD term on early epochs.
It is parameterised as
\begin{equation}
    \xi(n) = \left( \min\left( \frac{n}{0.4 N_\mathrm{fade}}, 1 \right) \right)^3,
\end{equation}
where $n$ is the current epoch, and $N_\mathrm{fade}$ is the number of epochs needed for this term to become unity.
To ensure that the output in the zero-padded sections remains close to zero we use $\xi$ to initialise these with a small amount of random noise, decaying over this same period.
This increases the activation of these neurons early on, forcing the network to learn that these must be adjusted towards 0 for all inputs.
Empirically we found that for the fade-in period of 800 epochs (over which $\xi$ increases to 1), a good choice for the loss weights was $w_\mathrm{pred}=4000$, $w_\mathrm{latent}=900$, and $w_\mathrm{rev}=1000$.
After the fade-in period the network was trained in blocks of 400 epochs, increasing the value of $w_\mathrm{pred}$ by 1000 for each block of these.
From 4,800 epochs to 12,000 epochs the network was trained in blocks of 600 epochs, with the value of $w_\mathrm{pred}$ being increased by the same amount.
These weights were all tuned empirically, and others were also found to yield good convergence, however we found it important that the L2 weight be a factor of 2 or more larger than the MMD weights or the forwards process would not reliably converge.

The gradients computed from these losses are used in conjunction with the Adam optimiser \citep{2014Kingma} with $\beta_1=\beta_2=0.8$ and $\epsilon=1\times10^{-6}$.
The $\beta_i$ terms control the decay rate for momentum of the first- and second-moment estimates of the gradients and $\epsilon$ simply prevents division by zero.
These gradients are clipped to a range of $\pm15$ to help further mitigate problems with exploding gradients.
This does not affect the final solution as the gradients will decrease as we approach a minimum.
The learning rate was initialised to $1.5\times10^{-3}$ and decays by a factor of $0.004^{1/1333}$ every 12 epochs.
Each minibatch contained 500 different samples and 20 minibatches were used per learning epoch.
The final model was selected based on its L2 performance for the forwards and backwards results on the validation set.
In this case the best performing model was the one saved after 11,400 epochs of training.



\subsection{Maximum Mean Discrepancy}\label{Sec:Mmd}

The content of this section draws primarily from \citet{Sriperumbudur2009}, \citet{2012Gretton}, \citet{Muandet2017}, and Gretton's lecture content, currently available online\footnote{\url{http://www.gatsby.ucl.ac.uk/~gretton/teaching.html}}.
% e.g. http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture1_whatIsRKHS.pdf

The maximum mean discrepancy (MMD) is a statistic for comparing two distributions, based on samples drawn from these, and computes the difference in expectations over functions in the unit ball of a reporducing kernel Hilbert space (RKHS).

A Hilbert space is a space in which an inner product is defined that is linear, symmetric, and the inner product of an element with itself is positive definite.
From this inner product a norm can be defined $||f|| = \sqrt{\langle f, f \rangle}$.
These spaces must also be Cauchy complete, implying that every Cauchy sequence (convergent sequence), must converge to a point in the space.

An RKHS $\mathcal{H}$ is then a Hilbert space of functions $f : \mathcal{X} \rightarrow \mathbb{R}$ for which the evaluation functional $\delta_x : f \mapsto f(x)$ is bounded and continuous\footnote{This can be generalised to functions $f : \mathcal{X} \rightarrow \mathbb{C}$.}.
The RKHS has the property that two functions that are close in norm in $\mathcal{H}$ are then pointwise close when evaluated anywhere over $\mathcal{X}$.
The kernel associated with this RKHS is a positive definite kernel $k : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ if there exists a map $\phi : \mathcal{X} \rightarrow \mathcal{H}$ such that $\forall\, x,y \in \mathcal{H}$
\begin{equation}
    k(x,y) = \langle \phi(x), \phi(y) \rangle_\mathcal{H}.
\end{equation}
The map $\phi$ is termed the feature map, and $\mathcal{H}$ is known as the feature space.

Let us define two probability distributions $P$ and $Q$ and draw an observation $X$ and $Y$ in an independent and identically distributed fashion from $P$ and $Q$ respectively.
The MMD is then defined by
\begin{equation}\label{Eq:MmdDef}
\begin{aligned}
    \mathrm{MMD}^2 &= ||\mu_P - \mu_Q ||_\mathcal{H}^2\\
    &= \langle \mu_P, \mu_P \rangle_\mathcal{H} +
       \langle \mu_Q, \mu_Q \rangle_\mathcal{H} -
       2 \langle \mu_P, \mu_Q \rangle_\mathcal{H},
\end{aligned}
\end{equation}
where $\mu_A$ represents the mean embedding of the distribution $A$ in the feature space, which is the expectation vector of the features of $\mathcal{H}$ evaluated for this distribution.
The kernel associated with $\mathcal{H}$ is \emph{characteristic} if the feature map is injective, and in this case the MMD is zero iff $P = Q$.
It was shown by \citet{Sriperumbudur2009} that all measurable and bounded strictly positive definite kernels are characteristic, and we therefore limit ourselves to this class.

$\mu_P$ can now be written in terms of the features of $\mathcal{H}$
\begin{equation}
    \mu_P = [\ldots \mathbb{E}_P[\phi_i(X)] \ldots],
\end{equation}
where $\mathbb{E}_P$ denotes the expected value of its argument with respect to $P$ and $\phi_i$ is the $i$th feature of the feature map (which may contain infinitely many features).
We can then write
% Muandet2017, p51, crossed with Gretton Madrid notes
\begin{equation}
    \langle \mu_P, \mu_P \rangle_\mathcal{H} = \langle \mathbb{E}_P [k(\cdot\, ,\, X)], \mathbb{E}_{P} [k(\cdot\, ,\, X^\prime)] \rangle_\mathcal{H} = \mathbb{E}_{P} [k(X, X^\prime)],
\end{equation}
where $X^\prime$ is an independently drawn copy of $X$ from $P$.
We define $Y^\prime$ analagously for $Y$ and $Q$ to then write \eqref{Eq:MmdDef} as
\begin{equation}
    \mathrm{MMD}^2 = ||\mu_P - \mu_Q|| ^2
                   = \mathbb{E}_P [k(X, X^\prime)] + \mathbb{E}_Q [k(Y, Y^\prime)] - 2 \mathbb{E}_{P,Q}[k(X,Y)].
\end{equation}
Here the first two terms compare the distributions for internal similarity, whereas the last compares the intra-distribution similarity.

For length $n$ finite observations $X$ and $Y$ we can expand this to provide an unbiased sample estimate
\begin{equation}
    \widehat{\mathrm{MMD}}_u^2 = \frac{1}{n(n-1)} \sum_{i\neq j} k(x_i, x_j) + \frac{1}{n(n-1)} \sum_{i\neq j} k(y_i, y_j) - \frac{2}{n^2} \sum_{i, j} k(x_i, y_j).
\end{equation}
As this statistic is unbiased and computed from a finite sample size, it can be negative when $P$ and $Q$ are similar distributions, despite being the definition of $\mathrm{MMD}^2$.
We therefore employ a biased estimate of the MMD which remains positive in all scenarios
\begin{equation}
    \widehat{\mathrm{MMD}}_b^2 = \frac{1}{n^2}\sum_{i, j}\left( k(x_i, x_j) + k(y_i, y_j) - 2k(x_i, x_j) \right).
\end{equation}
This biased form of the MMD is also more efficient to compute using the vectorised tensor operations present in machine learning frameworks like pytorch.

The choice of kernel used in the MMD will then determine how this statistic is able to distinguish between distributions.
We choose to employ the inverse multi-quadric (IMQ) kernel as used by \citet{2017Tolstikhin} and \citet{2018Ardizzone}
\begin{equation}
    k_\alpha(x, y) = \frac{\alpha^2}{\alpha^2 + ||x-y||_2^2}.
\end{equation}
This kernel meets the strictly positive definite criterion that is necessary for a characterisic kernel of an RKHS, but contains a free parameter $\alpha$.
We were unable to empirically determine a single value for $\alpha$ for which the MMD could reliably distinguish between the two distributions well enough to optimise over, however it was found that the value of the MMD for different values of $\alpha$ whilst retaining fixed $X$ and $Y$ peaked for a particular value of $\alpha$.
This approach was used to refine the MMD losses by updating $\alpha$ to the value for which the MMD was maximal every five epochs of training.
Modifying the MMD loss in this way ensures that it is most sensitive to the scale of differing features between our current distribution samples.

Minimising the forwards MMD loss then ensures the independence of data stored in the latent space and that the latent space takes a normal unit distribution form, whilst minimising the backwards MMD ensures that the distribution of atmospheres generated with different draws of the latent space is a plausible distribution when compared against the training set.

\subsection{Validation}

\begin{pycode}[Radynversion]
def logvel_to_vel(v):
    vSign = v / np.abs(v)
    vSign[np.isnan(vSign)] = 0
    vel = vSign * (10**np.abs(v) - 1.0)
    return vel

def find_range_in_schema(schema, name):
    nameIdx = 0
    for i, s in enumerate(schema):
        if s[0] == name:
            nameIdx = i
            break
    else:
        raise ValueError('Name %s not found in schema %s' % (name, repr(schema)))

    startLoc = 0
    for i in range(nameIdx):
        startLoc += schema[i][1]
    endLoc = startLoc + schema[nameIdx][1]
    return range(startLoc, endLoc)

with open(chRad.data_file('Validation/Schemas.pickle'), 'rb') as f:
    schemas = pickle.load(f)

forwards = np.load(chRad.data_file('Validation/ForwardsProcess.npz'))
reverse = np.load(chRad.data_file('Validation/ReverseProcess.npz'))
z = schemas['z']
zMm = schemas['z'] / 1e8
wls = schemas['wls']

fig, ax = plt.subplots(2, 2, #constrained_layout=True,
                       figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.8))
ax = ax.ravel()
ax = [ax[0], ax[0].twinx(), *ax[1:]]
ax[0].plot(zMm, forwards['xTrue'][0, 0])
ax[1].plot(zMm, forwards['xTrue'][0, 1], c='C1')
ax[2].plot(zMm, logvel_to_vel(forwards['xTrue'][0, 2]), c='C2')
ax[0].set_ylabel(r'$\log{n_e}$ [\si{\cm\tothe{-3}}]', c='C0')
ax[0].set_xlabel(r'$z$ [\si{\mega\metre}]')
ax[1].set_ylabel(r'$\log{T}$ [\si{\K}]', c='C1')
ax[2].set_ylabel(r'$v$ [\si{\kilo\metre\per\second}]', c='C2')
ax[2].set_xlabel(r'$z$ [\si{\mega\metre}]')


halphaCoords = find_range_in_schema(schemas['outSchema'], 'Halpha')
caCoords = find_range_in_schema(schemas['outSchema'], 'Ca8542')
haCentre = 0.5 * (wls[0][0] + wls[0][-1])
caCentre = 0.5 * (wls[1][0] + wls[1][-1])
ax[3].plot((wls[0] - haCentre) / 10, forwards['yTrue'][0, 0], c='C2')
ax[3].plot((wls[0] - haCentre) / 10, forwards['yzPred'][0, halphaCoords], '-.', c='C4')
ax[4].plot((wls[1] - caCentre) / 10, forwards['yTrue'][0, 1], c='C2', label='Ground Truth')
ax[4].plot((wls[1] - caCentre) / 10, forwards['yzPred'][0, caCoords], '-.', c='C4', label='Predicted')
ax[3].xaxis.set_major_locator(plt.MaxNLocator(6))
ax[4].xaxis.set_major_locator(plt.MaxNLocator(6))
fig.legend(loc='lower left', frameon=False, bbox_to_anchor=(0.42, 0.45))

ax[3].set_ylabel('Normalised Intensity')
ax[3].set_xlabel(r'$\Delta\lambda$ [\si{\nano\metre}]')
ax[4].set_xlabel(r'$\Delta\lambda$ [\si{\nano\metre}]')
ax[3].set_title(r'H$\alpha$')
ax[4].set_title(r'Ca\,\textsc{ii} \SI{854.2}{\nano\metre}')

fig.tight_layout()
latexFig = chRad.save_figure('RadynversionValidationForwards', fig, fext='.pgf')
latexFig.caption = 'Validation of the RADYNVERSION forwards process on unseen data. The upper row shows the atmospheric input, and the lower row the expected output along with the predicition from the network.'
\end{pycode}

\begin{pycode}[Radynversion]
from matplotlib.colors import LinearSegmentedColormap, PowerNorm
cmapNe = [(1.0,1.0,1.0,0.0), (*sns.color_palette()[0], 1.0)]
neColors = LinearSegmentedColormap.from_list('ne', cmapNe)
cmapTemp = [(1.0,1.0,1.0,0.0), (*sns.color_palette()[1], 1.0)]
tempColors = LinearSegmentedColormap.from_list('temp', cmapTemp)
cmapVel = [(1.0,1.0,1.0,0.0), (*sns.color_palette()[2], 1.0)]
velColors = LinearSegmentedColormap.from_list('vel', cmapVel)
powerNormIdx = 0.2

fig, ax = plt.subplots(1, 2, #constrained_layout=True,
                       figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.4))
ax = ax.ravel()
ax = [ax[0], ax[0].twinx(), *ax[1:]]

zEdges = [zMm[0] - 0.5 * (zMm[1] - zMm[0])]
for i in range(zMm.shape[0] - 1):
    zEdges.append(0.5 * (zMm[i] + zMm[i+1]))
zEdges.append(zMm[-1] + 0.5 * (zMm[-1] - zMm[-2]))

xPred = reverse['xPred']
xTrue = reverse['xTrue']

neEdges = np.linspace(8, 15, 101)
neIdxs = find_range_in_schema(schemas['inSchema'], 'ne')
tempEdges = np.linspace(3, 8, 101)
tempIdxs = find_range_in_schema(schemas['inSchema'], 'temperature')
velIdxs = find_range_in_schema(schemas['inSchema'], 'vel')
minVel = np.min(np.median(logvel_to_vel(xPred[:, velIdxs]), axis=0))
minVel = np.sign(minVel) * 2 * np.abs(minVel) if minVel <= 0 else 0.9 * minVel
maxVel = 2 * np.max(np.median(logvel_to_vel(xPred[:, velIdxs]), axis=0))
velEdges = np.linspace(minVel, maxVel, num=101)

ax[0].hist2d(np.concatenate([zMm] * xPred.shape[0]),
             xPred[:, neIdxs].reshape(-1),
             bins=(zEdges, neEdges), cmap=neColors, norm=PowerNorm(powerNormIdx), rasterized=True)
ax[1].hist2d(np.concatenate([zMm] * xPred.shape[0]),
             xPred[:, tempIdxs].reshape(-1),
             bins=(zEdges, tempEdges), cmap=tempColors, norm=PowerNorm(powerNormIdx), rasterized=True)
ax[0].plot(zMm, xTrue[0, 0], 'k--', linewidth=0.5)
ax[1].plot(zMm, xTrue[0, 1], 'k--', linewidth=0.5)
ax[0].set_xlim(None, 10.5)

ax[2].hist2d(np.concatenate([zMm] * xPred.shape[0]),
             logvel_to_vel(xPred[:, velIdxs].reshape(-1)),
             bins=(zEdges, velEdges), cmap=velColors, norm=PowerNorm(powerNormIdx), rasterized=True)
ax[2].plot(zMm, logvel_to_vel(xTrue[0, 2]), 'k--', linewidth=0.5)
ax[2].set_xlim(None, 10.5)


ax[0].set_ylabel(r'$\log{n_e}$ [\si{\cm\tothe{-3}}]', c=cmapNe[-1])
ax[0].set_xlabel(r'$z$ [\si{\mega\metre}]')
ax[1].set_ylabel(r'$\log{T}$ [\si{\K}]', c=cmapTemp[-1])
ax[2].set_ylabel(r'$v$ [\si{\kilo\metre\per\second}]', c=cmapVel[-1])
ax[2].set_xlabel(r'$z$ [\si{\mega\metre}]')

fig.tight_layout()
latexFig = chRad.save_figure('RadynversionValidationReverse', fig, fext='.pgf', dpi=500)
latexFig.caption = r'Validation of the Radynversion reverse process from the ground truth line profiles shown in Fig.~\ref{Fig:RadynversionValidationForwards}. The two-dimensional histograms show the probability density of the solution in each altitude node, and the black dashed lines the expected solution.'
\end{pycode}

\py[Radynversion]|chRad.get_figure('RadynversionValidationForwards')|
During and after training, the RADYNVERSION model is validated against the unseed validation data separated from the training set.
An example taken from the validation set showing the forwards process is presented in Fig.~\ref{Fig:RadynversionValidationForwards}.
The mean squared error in the scaled intensity at each wavelength point for the validation set is $5.74\times10^{-5}$ showing that the model can very accurately predict the scaled line profiles based on the atmospheric parameters for unseen atmospheres.

\py[Radynversion]|chRad.get_figure('RadynversionValidationReverse')|

Due to possible degeneracies it is harder to numerically evaluate the performance of the model's reverse process.
For each pair of line profiles we draw a large number of samples from the latent space and infer the predicted line profile from each latent space draw combined with the pair of line profiles.
This gives a large number of possible sets of atmospheric parameters which we plot on a two dimensional histogram to show the probability density of the solution in each altitude node.
This method then allows us to gauge possible degeneracies and establish the relative probabilities of different solutions.
An example validation inversion is shown in Fig.~\ref{Fig:RadynversionValidationReverse}, using the ground truth output line profiles shown in the forward validation process.
These histograms were generated from 10,000 draws latent space draws, and their values were gamma corrected (with $\gamma=\,$\py[Radynversion]|powerNormIdx|) to enhance the visibility of less probable solutions.
The overplotted black dashed lines show the ground truth solution for each of the atmospheric parameters, and overlap the peak density of the histogram extremely well.
The histogram remains very narrow in the lower atmosphere, but starts to expand above \SI{3}{\mega\metre}, where the solution is poorly constrained by the chromospheric lines in use here.
This effect is most visible on the velocity plot, but despite the increase in uncertaintly, the solutions remain accurate on the validation set due to the model's conditioning to the RADYN training data.

\subsection{Proof of Concept Results}

\begin{pycode}[Radynversion]
from astropy.io import fits
ca_file = chRad.data_file('crisp_l2_20140906_152724_8542_r00459.fits')
ha_file = chRad.data_file('crisp_l2_20140906_152724_6563_r00459.fits')

ha_central_idx = 7
ca_central_idx = 12
ha_hw = 1.4
ca_hw = 1.0 #1.0 because RADYN has a 1A half-width for calcium

# crisp = CRISP(ca_file=ca_file,ha_file=ha_file,rotate=True)
ca = fits.open(ca_file)
ha = fits.open(ha_file)

im_centre_ca = ca[0].data[12]
im_centre_ha = ha[0].data[7]
im_bwing_ca =  ca[0].data[4]
im_bwing_ha =  ha[0].data[0]
im_rwing_ca =  ca[0].data[20]
im_rwing_ha =  ha[0].data[14]
px_width = ca[0].header['CDELT1']
point_1_ha = np.array([30.72,37.47])
point_2_ha = np.array([50.37,3.73])
point_1_ca = np.array([30.22,36.47])
point_2_ca = np.array([49.87,2.73])
point_2_ca = np.array([35, 5])
point_2_ha = point_2_ca
# point_1_ha = np.array([53.5, 39.5])
# point_1_ha = np.array([49.9, 42.3]) #broken
# point_1_ha = np.array([50.27, 41.50]) # not too bad
# point_1_ha = np.array([49.191, 42.294]) # True one for Ha from paper.
point_1_ha = np.array([49.05, 42.19]) # Close.
point_1_ca = point_1_ha

fig, ax = plt.subplots(ncols=3,nrows=2,
figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.8),
constrained_layout=True, sharex=True, sharey=True)
sol_cm = 'Greys_r'
ax[0,0].imshow(im_bwing_ca,extent=[0,px_width*im_centre_ca.shape[1],0,px_width*im_centre_ca.shape[0]],origin="bottom",cmap=sol_cm)
ax[0,0].set_title(r"$\lambda=854.2-0.08$ nm")
ax[0,0].set_ylabel("y [\"]")
ax[0,0].plot(point_1_ca[0],point_1_ca[1],"wo")
ax[0,0].plot(point_2_ca[0],point_2_ca[1],"ws")
ax[0,1].imshow(im_centre_ca,extent=[0,px_width*im_centre_ca.shape[1],0,px_width*im_centre_ca.shape[0]],origin="bottom",cmap=sol_cm)
ax[0,1].set_title(r"$\lambda=854.2$ nm")
ax[0,1].plot(point_1_ca[0],point_1_ca[1],"wo")
ax[0,1].plot(point_2_ca[0],point_2_ca[1],"ws")
ax[0,2].imshow(im_rwing_ca,extent=[0,px_width*im_centre_ca.shape[1],0,px_width*im_centre_ca.shape[0]],origin="bottom",cmap=sol_cm)
ax[0,2].set_title(r"$\lambda=854.2+0.08$ nm")
ax[0,2].plot(point_1_ca[0],point_1_ca[1],"wo")
ax[0,2].plot(point_2_ca[0],point_2_ca[1],"ws")
ax[1,0].imshow(im_bwing_ha,extent=[0,px_width*im_centre_ca.shape[1],0,px_width*im_centre_ca.shape[0]],origin="bottom",cmap=sol_cm)
ax[1,0].set_title(r"$\lambda=656.3-0.14$ nm")
ax[1,0].set_ylabel("y [\"]")
ax[1,0].set_xlabel("x [\"]")
ax[1,0].plot(point_1_ha[0],point_1_ha[1],"wo")
ax[1,0].plot(point_2_ha[0],point_2_ha[1],"ws")
ax[1,1].imshow(im_centre_ha,extent=[0,px_width*im_centre_ca.shape[1],0,px_width*im_centre_ca.shape[0]],origin="bottom",cmap=sol_cm)
ax[1,1].set_title(r"$\lambda=656.3$ nm")
ax[1,1].set_xlabel("x [\"]")
ax[1,1].plot(point_1_ha[0],point_1_ha[1],"wo")
ax[1,1].plot(point_2_ha[0],point_2_ha[1],"ws")
ax[1,2].imshow(im_rwing_ha,extent=[0,px_width*im_centre_ca.shape[1],0,px_width*im_centre_ca.shape[0]],origin="bottom",cmap=sol_cm)
ax[1,2].set_title(r"$\lambda=656.3+0.14$ nm")
ax[1,2].set_xlabel("x [\"]")
ax[1,2].plot(point_1_ha[0],point_1_ha[1],"wo")
ax[1,2].plot(point_2_ha[0],point_2_ha[1],"ws")

lFig = chRad.save_figure('CrispPlot', fig, fext='.pgf', dpi=600)
lFig.caption = r'Observations of the the M1.1 flare from AR 12157 on 2019-09-06 just after flare onset. The upper row shows images in the \CaLine{} band, and the lower row shows equivalent images from the \Ha{} band. These two inverted pixels are marked by the square (off ribbon) and circle (on ribbon). Wavelengths in this figure use their values in air.'
\end{pycode}

\begin{pycode}[Radynversion]
from utils import intensity_vector, interp_to_radyn_grid, normalise
ca_1 = intensity_vector(ca, point_1_ca,coord_type="arcsec")
ha_1 = intensity_vector(ha, point_1_ha,coord_type="arcsec")
ca_2 = intensity_vector(ca, point_2_ca,coord_type="arcsec")
ha_2 = intensity_vector(ha, point_2_ha,coord_type="arcsec")

ca_centre_wvl = ca[0].header["TWAVE1"]
ca_wvls = ca[1].data
ha_centre_wvl = ha[0].header["TWAVE1"]
ha_wvls = ha[1].data

new_ca_1 = interp_to_radyn_grid(ca_1,ca_centre_wvl,ca_hw,ca_wvls)
new_ha_1 = interp_to_radyn_grid(ha_1,ha_centre_wvl,ha_hw,ha_wvls)
new_ca_2 = interp_to_radyn_grid(ca_2,ca_centre_wvl,ca_hw,ca_wvls)
new_ha_2 = interp_to_radyn_grid(ha_2,ha_centre_wvl,ha_hw,ha_wvls)

new_ca_1, new_ha_1 = normalise(new_ca_1,new_ha_1)
new_ca_2, new_ha_2 = normalise(new_ca_2,new_ha_2)

fig, ax = plt.subplots(2,2,
figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.7),
constrained_layout=True)
ax[0,0].plot(new_ca_1[0]/10 - ca_centre_wvl/10,new_ca_1[1])
ax[0,0].set_title(r"Ca\,\textsc{ii} \SI{854.2}{\nano\metre} for the circlular point")
ax[0,0].set_ylabel("Normalised Intensity")
ax[0,1].plot(new_ha_1[0]/10 - ha_centre_wvl/10,new_ha_1[1])
ax[0,1].set_title(r"H$\alpha$ for the circular point")
ax[1,0].plot(new_ca_2[0]/10 - ca_centre_wvl/10,new_ca_2[1])
ax[1,0].set_title(r"Ca\,\textsc{ii} \SI{854.2}{\nano\metre} for the square point")
ax[1,0].set_ylabel("Normalised Intensity")
ax[1,0].set_xlabel(r"$\Delta\lambda$ [nm]")
ax[1,1].plot(new_ha_2[0]/10 - ha_centre_wvl/10,new_ha_2[1])
ax[1,1].set_title(r"H$\alpha$ for the square point")
ax[1,1].set_xlabel(r"$\Delta\lambda$ [nm]")
for axi in ax.flat:
    axi.xaxis.set_major_locator(plt.MaxNLocator(5))
    axi.yaxis.set_major_locator(plt.MaxNLocator(5))
lFig = chRad.save_figure('LineProfilesToInvert', fig, fext='.pgf')
lFig.caption = r'The spectral line profiles of \CaLine{} and \Ha{} for the two pixels marked in Fig.~\ref{Fig:CrispPlot}.'
\end{pycode}


% NOTE(cmo): To further adjust these in future go back to hephaistos and use the single_pixel_inversions notebook in ~/RadynversionRef.
\begin{pycode}[Radynversion]
import utils
with open(chRad.data_file('InversionResults.pickle'), 'rb') as f:
    res = pickle.load(f)

fig = utils.inversion_plots(res['flare'], utils.z, new_ca_1, new_ha_1, rasterizeHists=True, figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.4), powerNormIdx=0.2)
lFig = chRad.save_figure('InversionResultsFlare', fig, fext='.pgf', dpi=500)
lFig.caption = r'Inversion results for the on ribbon pixel. The histograms represent the probability density for the solution in each altitude node, and the median solution is shown with the dashed black lines.'

fig = utils.inversion_plots(res['quiet'], utils.z, new_ca_1, new_ha_1, rasterizeHists=True, figsize=texfigure.figsize(pytex, scale=1, height_ratio=0.4), powerNormIdx=0.2)
lFig = chRad.save_figure('InversionResultsQuiet', fig, fext='.pgf', dpi=500)
lFig.caption = r'Inversion results for the off ribbon pixel. The histograms represent the probability density for the solution in each altitude node, and the median solution is shown with the dashed black lines.'
\end{pycode}

As a proof of concept the RADYNVERSION model was applied to the two-ribbon M1.1 flare SOL 20140906T17:09 observed with the CRISP instrument in \Ha{} and \CaLine{}.
This event ocurred in NOAA AR 12157 with heliocentric coordinates (-732", -302").
This data was preprocessed using the CRISPRED pipeline \citep{2015dlcr} and is publicly available in the F-CHROMA database.

\py[Radynversion]|chRad.get_figure('CrispPlot')|

Due to the formation heights of these spectral lines, our interest is primarily focused on the region below $\sim$\SI{2}{\mega\metre}.
Fig.~\ref{Fig:CrispPlot} shows CRISP images in the blue wing, line core, and red wing for both of the spectral lines respectively.
These images were taken just after flare onset.
Two pixels are marked, a circle on the flare ribbon, and a square far from the flare, in a much quieter region.
The spectral line profiles from these two pixels are shown in Fig.~\ref{Fig:LineProfilesToInvert}.
For the circular point both lines are strongly in emssion whereas they are broad absorption lines for the square point.
These lines from these pixels have then been inverted using RADYNVERSION, with 20,000 latent space draws each.
The results of these inversions are shown in Figs~\ref{Fig:InversionResultsFlare}~and~
\ref{Fig:InversionResultsQuiet} for the circular and square points respectively.
Performing these inversions with 20,000 draws each takes $\sim$\SI{893}{\milli\second} on an NVIDIA GTX 1050 Ti GPU.
As there is no ground truth solution for this data we instead overplot the median solution at each altitude node.

\py[Radynversion]|chRad.get_figure('LineProfilesToInvert')|
\py[Radynversion]|chRad.get_figure('InversionResultsFlare')|
\py[Radynversion]|chRad.get_figure('InversionResultsQuiet')|

Substantial analysis, detailed in the original paper, was undertaken by my co-authors, which I will briefly summarise here.
The inverted pixels were found to be consistent with previous analyses.
For example, forward modelling by \citet{Kuridze2015} suggests that the \Ha{} line profile forms below \SI{1.2}{\mega\metre}, with the core forming towards the top of this region, and the line wings forming below \SI{0.95}{\mega\metre}.
In this observation the \Ha{} line profile is asymmetric in favour of the red wing, and from the inversions we see that there is a slight upflow in the region where the wings are formed.
This is likely chromospheric evaporation in this region causing an increase in the opacity of the blue wing, leading in turn to more intensity in the red wing.
For the off-ribbon pixel we note that both line profiles are very broad.
The inverted velocity field contains a significant number of small magnitude oscillations which we believe are due to RADYN's conservative assumption of a \SI{2}{\kilo\metre\per\second} microturbulent velocity throughout the atmosphere.
If the network has learnt that bulk plasma flows can shift the position of the line core (i.e. the majority of the opacity in a line), then it is reasonable to suggest that these represent our model's attempt to broaden th line profiles.
Recent studies have shown that significantly higher microturbulent velocities are needed to explain the nonthermal broadenings observed in chromospheric plages \citep[\SIrange{6}{7}{\kilo\metre\per\second};][]{2015Carlsson}.

\subsection{Discussion}

The RADYNVERSION invertible neural network presented in this chapter represents a novel method for investigating the atmospheric properties of observed events and lifts many of the restrictions that previously made inversions of flaring line profiles infeasible.
This is achieved by the union of machine learning and a large body of RHD simulations.
The implementation is currently a proof of concept but shows much promise, agreeing with previous investigations, and can easily be extended to other spectral lines and atmospheric parameters.
With the addition of more advanced forwards models this technique could be applied to inference of the chromospheric magnetic field from full Stokes observations.
Inversions using this method are fast and robust; the effort of ``exploring'' the latent space that is common in regression based codes has effectively been replaced with an up-front cost in the training process.
Once the model is trained, taking multiple draws from the latent space replicates the exploration, but extremely rapidly, due the previous training effort, which is then shared between every application of the model (in the traditional 1.5D inversion technique, where every column is treated independently, a lot of this work is replicated).
These latent draws additionally serve to provide an estimate of the uncertainty on the inferred parameters.
Potential future extensions to this model include incorporating the concept of time-dependence in a robust fashion so as to condition the solution from previous observations, and increasing the number of atmospheric parameters inferred.

The RADYNVERSION approach has the potential to provide new insight into the structure of the flaring chromosphere and allow investigation of complete observation fields of view in a timely fashion, allowing researchers to leverage the full capabilities of next-generation solar telescopes such as DKIST.